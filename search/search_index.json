{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#heiplanet-data-a-data-processing-pipeline-for-climate-and-population-data","title":"heiplanet-data: A data processing pipeline for climate and population data","text":"<p>A data processing pipeline for Copernicus and Eurostat data. Please visit the documentation for further information.</p> <p>Further information:</p> <ul> <li>About: Information about the project, research group, and development group</li> <li>Data: Used data sources and conventions</li> <li>Datalake: Data lake architecture and files</li> <li>Tutorials: Tutorials how to use heiplanet-data (Tutorial A: Download data (open in colab), Tutorial B: Process data (open in colab), Tutorial C: Aggregate data (open in colab))</li> <li>Issues: Some notes for running the package and further issues</li> <li>Contact: Contact information of relevant groups</li> <li>License: License information</li> </ul>"},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":"<p>TBU.</p> <p>About the project and group info.</p> <p>Heidelberg Planetary Health Hub (Hei-Planet)</p> <p>Scientific Software Center (SSC)</p>"},{"location":"contact/","title":"contact","text":""},{"location":"contact/#contact","title":"Contact","text":"<p>If you have any questions or would like to contribute to the project, feel free to contact us at ssc@iwr.uni-heidelberg.de or another@email.</p>"},{"location":"data/","title":"Data","text":""},{"location":"data/#data","title":"Data","text":"<p>The supported models rely on data from the Copernicus's CDS, Eurostat's NUTS definition, and ISIMIP's population data.</p>"},{"location":"data/#copernicus-data","title":"Copernicus Data","text":"<p>The CDS's ERA5-Land monthly dataset is currently being used for now. You can either download the data directly from CDS website or use the provided Python script, <code>inout module</code>.</p> <p>For the latter option, please set up the CDS API as outlined below and take note of the naming convention used for the downloaded files.</p>"},{"location":"data/#set-up-cds-api","title":"Set up CDS API","text":"<p>To use  CDS API for downloading data, you need to first create an account on CDS to obtain your personal access token.</p> <p>Create a <code>.cdsapirc</code> file containing your personal access token by following this instruction.</p>"},{"location":"data/#naming-convention","title":"Naming convention","text":"<p>The filenames of the downloaded netCDF files follow this structure: </p><pre><code>{base_name}_{year_str}_{month_str}_{day_str}_{time_str}_{var_str}_{ds_type}_{area_str}_raw.{ext}\n</code></pre><p></p> <ul> <li><code>base_name</code> is <code>\"era5_data\"</code>,</li> <li>For list of numbers, i.e. years/months/days/times, the rule below is applied<ul> <li>If the values are continuous, the string representation is a concatenate of <code>min</code> and <code>max</code> values, separated by <code>-</code></li> <li>Otherwise, the string is a join of all values, separated by <code>_</code></li> <li>However, if there are more than 5 values, we only keep the first 5 ones and replace the rest by <code>\"_etc\"</code></li> <li>If the values are empty (e.g. no days or times in the download request), their string representation and the corresponding separator (i.e. <code>\"_\"</code>) are omitted from the file name.</li> </ul> </li> <li><code>year_str</code> is the string representation of list of years using the rule above.</li> <li>Similarly for <code>month_str</code>. However, if the download requests all 12 months, <code>month_str</code> would be <code>\"allm\"</code></li> <li><code>day_str</code> and <code>time_str</code> follows the same pattern, assuming that a month has at most 31 days (<code>\"alld\"</code>) and a day has at most 24 hours (<code>\"allt\"</code>).<ul> <li>Special case: if data is downloaded at time <code>00:00</code> per day only, <code>time_str</code> would be <code>\"midnight\"</code> (e.g. precipitation data for P-model)</li> </ul> </li> <li>For <code>var_str</code>, each variable has an abbreviation derived by the first letter of each word in the variable name (e.g. <code>tp</code> for <code>total precipitation</code>).<ul> <li>All abbreviations are then concatenated by <code>_</code></li> <li>If this concatenated string is longer than 30 characters, we only keep the first 2 characters and replace the the rest by <code>\"_etc\"</code></li> </ul> </li> <li>As for <code>ds_type</code>:<ul> <li>If the file was downloaded from a monthly dataset, <code>\"monthly\"</code> is set to <code>ds_type</code>. This means the data is recorded only on the first day of each month.</li> <li>For other datasets, when data is downloaded only at midnight (<code>time_str</code> = <code>\"midnight\"</code>), the ds_type is <code>\"daily\"</code>, meaning one data record for one day of each month.</li> <li><code>ds_type</code> would be an empty string in other cases, i.e. multiple data records for each day of a month.</li> </ul> </li> <li>For <code>area_str</code>, if the downloaded data is only for an area of the grid (instead of the whole map), <code>\"area\"</code> would represent for <code>area_str</code>.</li> <li>If the part before <code>\"_raw\"</code> is longer than 100 characters, only the first 100 characters are kept and the rest is replaced by <code>\"_etc\"</code></li> <li><code>\"_raw\"</code> is added at the end to indicate that the file is raw data</li> <li>Extension <code>ext</code> of the file can be <code>.nc</code> or <code>.grib</code></li> <li>If any of these fields (from <code>year_str</code> to <code>area_str</code>) are missing from the download request, the corresponding string and the preceding <code>_</code> are removed from the file name.</li> </ul>"},{"location":"data/#special-case","title":"Special case","text":"<p>As for total precipitation data downloaded from dataset <code>ERA5-Land hourly data from 1950 to present</code>, the file name is structured as:</p> <pre><code>{base_name}_{start_date}-{end_date}_{time_str}_{var_str}_{ds_type}_{area_str}_raw.{ext}\n</code></pre> <p>In this case, <code>time_str</code> is <code>\"midnight\"</code> and <code>ds_type</code> is <code>\"daily\"</code>.</p>"},{"location":"data/#eurostats-nuts-definition","title":"Eurostat's NUTS definition","text":"<p>The regions are set here and corresponding shapefiles can be downloaded here.</p> <p>For downloading, please choose:</p> <ul> <li>The latest year from NUTS year,</li> <li>File format: <code>SHP</code>,</li> <li>Geometry type: <code>Polygons (RG)</code>,</li> <li>Scale: <code>20M</code></li> <li>CRS: <code>EPSG: 4326</code></li> </ul> Note <ul> <li>After downloading the file, unzip it to access the root folder containing the NUTS data (e.g. folder named <code>NUTS_RG_20M_2024_4326.shp</code>)<ul> <li>Inside the unzipped folder, there are five different shapefiles, which are all required to display and extract the NUTS regions data. <pre><code>shape data folder\n|____.shp file: geometry data (e.g. polygons)\n|____.shx file: index for geometry data\n|____.dbf file: attribute data for each NUTS region (e.g NUTS name, NUTS ID)\n|____.prj file: information on CRS\n|____.cpg file: character encoding data\n</code></pre></li> </ul> </li> <li>These NUTS definition files are for Europe only.</li> <li>If a country does not have NUTS level \\(x \\in [1,3]\\), the corresponding data for these levels is excluded from the shapefiles.</li> </ul>"},{"location":"data/#nuts_id-explanation","title":"<code>NUTS_ID</code> explanation:","text":"<ul> <li>Structure of <code>NUTS_ID</code>: <code>&lt;country&gt;&lt;level&gt;</code></li> <li><code>country</code>: 2 letters, representing name of a country, e.g. DE</li> <li><code>level</code>: 0 to 3 letters or numbers, signifying the level of the NUTS region</li> </ul>"},{"location":"data/#isimip-data","title":"ISIMIP Data","text":"<p>To download population data, please perform the following steps:</p> <ul> <li>go to ISIMIP website</li> <li>search <code>population</code> from the search bar</li> <li>choose simulation round <code>ISIMIP3a</code></li> <li>click <code>Input Data</code> -&gt; <code>Direct human forcing</code> -&gt; <code>Population data</code> -&gt; <code>histsoc</code></li> <li>choose <code>population_histsoc_30arcmin_annual</code></li> <li>download file <code>population_histsoc_30arcmin_annual_1901_2021.nc</code></li> </ul>"},{"location":"datalake/","title":"Data Lake Backend","text":""},{"location":"datalake/#data-lake-backend","title":"Data Lake Backend","text":""},{"location":"datalake/#data-flowchart","title":"Data flowchart","text":"<p>Downloaded data files will go through preprocessing steps as follows:</p> <p></p>"},{"location":"datalake/#data-lake-architecture","title":"Data Lake architecture","text":"<p>The data lake is separated into bronze (raw data), silver (preprocessed data), and gold (processed data). The data preprocessing package deals with levels bronze and silver. The data is stored in <code>.data_onehealth_db/bronze</code> and <code>.data_onehealth_db/silver</code>.</p>"},{"location":"issues/","title":"Issues","text":""},{"location":"issues/#issues","title":"Issues","text":""},{"location":"issues/#address-deprecationwarning-of-the-mkdocs-jupyter-plugin","title":"Address DeprecationWarning of the mkdocs-jupyter plugin","text":"<p>While using <code>mkdocs serve</code> you might get warning as:</p> <p>DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs given by the platformdirs library.  To remove this warning and see the appropriate new directories, set the environment variable <code>JUPYTER_PLATFORM_DIRS=1</code> and then run <code>jupyter --paths</code>.</p> <p>To address this warning temporarily for every session:</p> bashpowershell <pre><code>export JUPYTER_PLATFORM_DIRS=1\nmkdocs serve\n</code></pre> <pre><code>$env:JUPYTER_PLATFORM_DIRS=1\nmkdocs serve\n</code></pre> <p>Or permanently by saving the variable value into your conda environment</p> <p></p><pre><code>conda activate your_env\nconda env config vars set JUPYTER_PLATFORM_DIRS=1 #(1)!\n</code></pre><p></p> <ol> <li>reactivate <code>your_env</code> would be needed</li> </ol>"},{"location":"issues/#other-issues","title":"Other issues","text":"<p>Other issues are listed here</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2025 SSC</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>heiplanet_data<ul> <li>inout</li> <li>preprocess</li> <li>script</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/inout/","title":"inout","text":""},{"location":"reference/inout/#heiplanet_datainout-module","title":"heiplanet_data.inout module","text":""},{"location":"reference/inout/#heiplanet_data.inout","title":"heiplanet_data.inout","text":"<p>Functions:</p> <ul> <li> <code>download_data</code>             \u2013              <p>Download data from Copernicus's CDS using the cdsapi.</p> </li> <li> <code>download_total_precipitation_from_hourly_era5_land</code>             \u2013              <p>Download total precipitation data from hourly ERA5-Land dataset.</p> </li> <li> <code>get_filename</code>             \u2013              <p>Get file name based on dataset name, base name, years, months and area.</p> </li> <li> <code>save_to_netcdf</code>             \u2013              <p>Save data to a NetCDF file.</p> </li> </ul>"},{"location":"reference/inout/#heiplanet_data.inout.download_data","title":"download_data","text":"<pre><code>download_data(output_file, dataset, request)\n</code></pre> <p>Download data from Copernicus's CDS using the cdsapi.</p> <p>Parameters:</p> <ul> <li> <code>output_file</code>               (<code>Path</code>)           \u2013            <p>The path to the output file where data will be saved.</p> </li> <li> <code>dataset</code>               (<code>str</code>)           \u2013            <p>The name of the dataset to download.</p> </li> <li> <code>request</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary containing the request parameters.</p> </li> </ul>"},{"location":"reference/inout/#heiplanet_data.inout.download_total_precipitation_from_hourly_era5_land","title":"download_total_precipitation_from_hourly_era5_land","text":"<pre><code>download_total_precipitation_from_hourly_era5_land(start_date, end_date, area=None, out_dir=Path('.'), base_name='era5_data', data_format='netcdf', ds_name='reanalysis-era5-land', coord_name='valid_time', var_name='total_precipitation', clean_tmp_files=False)\n</code></pre> <p>Download total precipitation data from hourly ERA5-Land dataset. Due to the nature of this dataset, value at 00:00 is total precipitation of the previous day. Therefore, to get total precipitation for the given range, We need to download data for the given range shifted by 1 day forward, then shift the time value back by 1 day after downloading.</p> <p>Parameters:</p> <ul> <li> <code>start_date</code>               (<code>str</code>)           \u2013            <p>Start date in \"YYYY-MM-DD\" format.</p> </li> <li> <code>end_date</code>               (<code>str</code>)           \u2013            <p>End date in \"YYYY-MM-DD\" format.</p> </li> <li> <code>area</code>               (<code>List[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Geographical area [North, West, South, East]. Default is None (global).</p> </li> <li> <code>out_dir</code>               (<code>Path</code>, default:                   <code>Path('.')</code> )           \u2013            <p>Output directory to save the downloaded file. Default is current directory.</p> </li> <li> <code>base_name</code>               (<code>str</code>, default:                   <code>'era5_data'</code> )           \u2013            <p>Base name for the file. Default is \"era5_data\".</p> </li> <li> <code>data_format</code>               (<code>str</code>, default:                   <code>'netcdf'</code> )           \u2013            <p>Data format (e.g., \"netcdf\", \"grib\"). Default is \"netcdf\".</p> </li> <li> <code>ds_name</code>               (<code>str</code>, default:                   <code>'reanalysis-era5-land'</code> )           \u2013            <p>Dataset name. Default is \"reanalysis-era5-land\". Only modify this if CDS changes the name of the dataset.</p> </li> <li> <code>coord_name</code>               (<code>str</code>, default:                   <code>'valid_time'</code> )           \u2013            <p>Name of the time coordinate in the dataset. Default is \"valid_time\". Only modify this if CDS changes the name of the coordinate.</p> </li> <li> <code>var_name</code>               (<code>str</code>, default:                   <code>'total_precipitation'</code> )           \u2013            <p>Name of the data variable. Default is \"total_precipitation\". Only modify this if CDS changes the name of the variable.</p> </li> <li> <code>clean_tmp_files</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag to indicate if temporary files should be deleted after processing. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The path to the downloaded file.</p> </li> </ul>"},{"location":"reference/inout/#heiplanet_data.inout.get_filename","title":"get_filename","text":"<pre><code>get_filename(ds_name, data_format, years, months, days=None, times=None, has_area=False, base_name='era5_data', variables=['2m_temperature'])\n</code></pre> <p>Get file name based on dataset name, base name, years, months and area.</p> <p>Parameters:</p> <ul> <li> <code>ds_name</code>               (<code>str</code>)           \u2013            <p>Dataset name.</p> </li> <li> <code>data_format</code>               (<code>str</code>)           \u2013            <p>Data format (e.g., \"netcdf\", \"grib\").</p> </li> <li> <code>years</code>               (<code>List[str] | None</code>)           \u2013            <p>List of years.</p> </li> <li> <code>months</code>               (<code>List[str] | None</code>)           \u2013            <p>List of months.</p> </li> <li> <code>days</code>               (<code>List[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of days.</p> </li> <li> <code>times</code>               (<code>List[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of times.</p> </li> <li> <code>has_area</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag indicating if area is included.</p> </li> <li> <code>base_name</code>               (<code>str</code>, default:                   <code>'era5_data'</code> )           \u2013            <p>Base name for the file. Default is \"era5_data\".</p> </li> <li> <code>variables</code>               (<code>List[str]</code>, default:                   <code>['2m_temperature']</code> )           \u2013            <p>List of variables. Default is [\"2m_temperature\"].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Generated file name.</p> </li> </ul>"},{"location":"reference/inout/#heiplanet_data.inout.save_to_netcdf","title":"save_to_netcdf","text":"<pre><code>save_to_netcdf(data, filename, encoding=None)\n</code></pre> <p>Save data to a NetCDF file.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataArray</code>)           \u2013            <p>Data to be saved.</p> </li> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>The name of the output NetCDF file.</p> </li> <li> <code>encoding</code>               (<code>Dict</code>, default:                   <code>None</code> )           \u2013            <p>Encoding options for the NetCDF file.</p> </li> </ul>"},{"location":"reference/preprocess/","title":"preprocess","text":""},{"location":"reference/preprocess/#heiplanet_datapreprocess-module","title":"heiplanet_data.preprocess module","text":""},{"location":"reference/preprocess/#heiplanet_data.preprocess","title":"heiplanet_data.preprocess","text":"<p>Functions:</p> <ul> <li> <code>adjust_longitude_360_to_180</code>             \u2013              <p>Adjust longitude from 0-360 to -180-180.</p> </li> <li> <code>aggregate_data_by_nuts</code>             \u2013              <p>Aggregate data from a NetCDF file by NUTS regions, data variable names, and time.</p> </li> <li> <code>align_lon_lat_with_popu_data</code>             \u2013              <p>Align longitude and latitude coordinates with population data    of the same resolution.</p> </li> <li> <code>convert_360_to_180</code>             \u2013              <p>Convert longitude from 0-360 to -180-180.</p> </li> <li> <code>convert_m_to_mm</code>             \u2013              <p>Convert precipitation from meters to millimeters.</p> </li> <li> <code>convert_m_to_mm_with_attributes</code>             \u2013              <p>Convert precipitation from meters to millimeters and keep attributes.</p> </li> <li> <code>convert_to_celsius</code>             \u2013              <p>Convert temperature from Kelvin to Celsius.</p> </li> <li> <code>convert_to_celsius_with_attributes</code>             \u2013              <p>Convert temperature from Kelvin to Celsius and keep attributes.</p> </li> <li> <code>downsample_resolution</code>             \u2013              <p>Downsample the resolution of a dataset.</p> </li> <li> <code>preprocess_data_file</code>             \u2013              <p>Preprocess the dataset based on provided settings.</p> </li> <li> <code>rename_coords</code>             \u2013              <p>Rename coordinates in the dataset based on a mapping.</p> </li> <li> <code>resample_resolution</code>             \u2013              <p>Resample the grid of a dataset to a new resolution.</p> </li> <li> <code>shift_time</code>             \u2013              <p>Shift the time coordinate of a dataset by a specified timedelta.</p> </li> <li> <code>truncate_data_by_time</code>             \u2013              <p>Truncate data from a specific start date to an end date. Both dates are inclusive.</p> </li> <li> <code>upsample_resolution</code>             \u2013              <p>Upsample the resolution of a dataset.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>CRS</code>           \u2013            </li> <li> <code>T</code>           \u2013            </li> <li> <code>warn_positive_resolution</code>           \u2013            </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.CRS","title":"CRS  <code>module-attribute</code>","text":"<pre><code>CRS = 4326\n</code></pre>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T', bound=Union[float64, DataArray])\n</code></pre>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.warn_positive_resolution","title":"warn_positive_resolution  <code>module-attribute</code>","text":"<pre><code>warn_positive_resolution = 'New resolution must be a positive number.'\n</code></pre>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.adjust_longitude_360_to_180","title":"adjust_longitude_360_to_180","text":"<pre><code>adjust_longitude_360_to_180(dataset, limited_area=False, lon_name='longitude')\n</code></pre> <p>Adjust longitude from 0-360 to -180-180.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset with longitude in 0-360 range.</p> </li> <li> <code>limited_area</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag indicating if the dataset is a limited area. Default is False.</p> </li> <li> <code>lon_name</code>               (<code>str</code>, default:                   <code>'longitude'</code> )           \u2013            <p>Name of the longitude variable in the dataset. Default is \"longitude\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset with longitude adjusted to -180-180 range.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.aggregate_data_by_nuts","title":"aggregate_data_by_nuts","text":"<pre><code>aggregate_data_by_nuts(netcdf_files, nuts_file, normalize_time=True, output_dir=None)\n</code></pre> <p>Aggregate data from a NetCDF file by NUTS regions, data variable names, and time. The aggregated data is saved to a NetCDF file with coordinates \"NUTS_ID\", \"time\", and data variables include aggregated data variables.</p> <p>Parameters:</p> <ul> <li> <code>netcdf_files</code>               (<code>dict[str, tuple[Path, dict | None]]</code>)           \u2013            <p>Dictionary of NetCDF files. Keys are dataset names and values are tuples of (file path, agg_dict). The agg_dict can contain aggregation options for each data variable. For example, {\"t2m\": \"mean\", \"tp\": \"sum\"}. If agg_dict is None, default aggregation (i.e. mean) is used. NetCDF files must contain \"latitude\", \"longitude\", and \"time\" coordinates.</p> </li> <li> <code>nuts_file</code>               (<code>Path</code>)           \u2013            <p>Path to the NUTS regions shape file. The shape file has columns such as \"NUTS_ID\" and \"geometry\".</p> </li> <li> <code>normalize_time</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize time to the beginning of the day. e.g. 2025-10-01T12:00:00 becomes 2025-10-01T00:00:00. Default is True.</p> </li> <li> <code>output_dir</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to save the aggregated NetCDF file. If None, the output file is saved in the same directory as the NUTS file. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>Path</code> )          \u2013            <p>Path to the aggregated NetCDF file.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.align_lon_lat_with_popu_data","title":"align_lon_lat_with_popu_data","text":"<pre><code>align_lon_lat_with_popu_data(dataset, expected_longitude_max=float64(179.75), lat_name='latitude', lon_name='longitude')\n</code></pre> <p>Align longitude and latitude coordinates with population data    of the same resolution. This function is specifically designed to ensure that the longitude and latitude coordinates in the dataset match the expected values used in population data, which are: - Longitude: -179.75 to 179.75, 720 points - Latitude: 89.75 to -89.75, 360 points</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset with longitude and latitude coordinates.</p> </li> <li> <code>expected_longitude_max</code>               (<code>float64</code>, default:                   <code>float64(179.75)</code> )           \u2013            <p>Expected maximum longitude after adjustment. Default is np.float64(179.75).</p> </li> <li> <code>lat_name</code>               (<code>str</code>, default:                   <code>'latitude'</code> )           \u2013            <p>Name of the latitude coordinate. Default is \"latitude\".</p> </li> <li> <code>lon_name</code>               (<code>str</code>, default:                   <code>'longitude'</code> )           \u2013            <p>Name of the longitude coordinate. Default is \"longitude\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset with adjusted longitude and latitude coordinates.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.convert_360_to_180","title":"convert_360_to_180","text":"<pre><code>convert_360_to_180(longitude)\n</code></pre> <p>Convert longitude from 0-360 to -180-180.</p> <p>Parameters:</p> <ul> <li> <code>longitude</code>               (<code>T</code>)           \u2013            <p>Longitude in 0-360 range.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code> (              <code>T</code> )          \u2013            <p>Longitude in -180-180 range.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.convert_m_to_mm","title":"convert_m_to_mm","text":"<pre><code>convert_m_to_mm(precipitation)\n</code></pre> <p>Convert precipitation from meters to millimeters.</p> <p>Parameters:</p> <ul> <li> <code>precipitation</code>               (<code>T</code>)           \u2013            <p>Precipitation in meters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code> (              <code>T</code> )          \u2013            <p>Precipitation in millimeters.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.convert_m_to_mm_with_attributes","title":"convert_m_to_mm_with_attributes","text":"<pre><code>convert_m_to_mm_with_attributes(dataset, inplace=False, var_name='tp')\n</code></pre> <p>Convert precipitation from meters to millimeters and keep attributes.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset containing precipitation in meters.</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, modify the original dataset. If False, return a new dataset. Default is False.</p> </li> <li> <code>var_name</code>               (<code>str</code>, default:                   <code>'tp'</code> )           \u2013            <p>Name of the precipitation variable in the dataset. Default is \"tp\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset with precipitation converted to millimeters.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.convert_to_celsius","title":"convert_to_celsius","text":"<pre><code>convert_to_celsius(temperature_kelvin)\n</code></pre> <p>Convert temperature from Kelvin to Celsius.</p> <p>Parameters:</p> <ul> <li> <code>temperature_kelvin</code>               (<code>T</code>)           \u2013            <p>Temperature in Kelvin, accessed through t2m variable in the dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code> (              <code>T</code> )          \u2013            <p>Temperature in Celsius.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.convert_to_celsius_with_attributes","title":"convert_to_celsius_with_attributes","text":"<pre><code>convert_to_celsius_with_attributes(dataset, inplace=False, var_name='t2m')\n</code></pre> <p>Convert temperature from Kelvin to Celsius and keep attributes.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset containing temperature in Kelvin.</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, modify the original dataset. If False, return a new dataset. Default is False.</p> </li> <li> <code>var_name</code>               (<code>str</code>, default:                   <code>'t2m'</code> )           \u2013            <p>Name of the temperature variable in the dataset. Default is \"t2m\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset with temperature converted to Celsius.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.downsample_resolution","title":"downsample_resolution","text":"<pre><code>downsample_resolution(dataset, new_resolution=0.5, lat_name='latitude', lon_name='longitude', agg_funcs=None, agg_map=None)\n</code></pre> <p>Downsample the resolution of a dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset to change resolution.</p> </li> <li> <code>new_resolution</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>New resolution in degrees. Default is 0.5.</p> </li> <li> <code>lat_name</code>               (<code>str</code>, default:                   <code>'latitude'</code> )           \u2013            <p>Name of the latitude coordinate. Default is \"latitude\".</p> </li> <li> <code>lon_name</code>               (<code>str</code>, default:                   <code>'longitude'</code> )           \u2013            <p>Name of the longitude coordinate. Default is \"longitude\".</p> </li> <li> <code>agg_funcs</code>               (<code>Dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Aggregation functions for each variable. If None, default aggregation (i.e. mean) is used. Default is None.</p> </li> <li> <code>agg_map</code>               (<code>Dict[str, Callable[[Any], float]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Mapping of string to aggregation functions. If None, default mapping is used. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset with changed resolution.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.preprocess_data_file","title":"preprocess_data_file","text":"<pre><code>preprocess_data_file(netcdf_file, source='era5', settings='default', new_settings=None, unique_tag=None)\n</code></pre> <p>Preprocess the dataset based on provided settings. If the settings path is \"default\", use the default settings of the source. The settings and preprocessed files are saved in the directory, which is specified by the settings file and unique number.</p> <p>Parameters:</p> <ul> <li> <code>netcdf_file</code>               (<code>Path</code>)           \u2013            <p>Path to the NetCDF file to preprocess.</p> </li> <li> <code>source</code>               (<code>Literal['era5', 'isimip']</code>, default:                   <code>'era5'</code> )           \u2013            <p>Source of the data. Defaults to \"era5\".</p> </li> <li> <code>settings</code>               (<code>Path | str</code>, default:                   <code>'default'</code> )           \u2013            <p>Path to the settings file or \"default\" for default settings.</p> </li> <li> <code>new_settings</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional settings to overwrite defaults. Defaults to None.</p> </li> <li> <code>unique_tag</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Unique tag to append to the output file name and settings file. Defaults to None.</p> </li> </ul> <p>Returns:     Tuple[xr.Dataset, str]: Preprocessed dataset and         the name of the preprocessed file.</p>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.rename_coords","title":"rename_coords","text":"<pre><code>rename_coords(dataset, coords_mapping)\n</code></pre> <p>Rename coordinates in the dataset based on a mapping.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset with coordinates to rename.</p> </li> <li> <code>coords_mapping</code>               (<code>dict</code>)           \u2013            <p>Mapping of old coordinate names to new names.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: A new dataset with renamed coordinates.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.resample_resolution","title":"resample_resolution","text":"<pre><code>resample_resolution(dataset, new_resolution=0.5, lat_name='latitude', lon_name='longitude', agg_funcs=None, agg_map=None, expected_longitude_max=float64(179.75), method_map=None)\n</code></pre> <p>Resample the grid of a dataset to a new resolution.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset to resample.</p> </li> <li> <code>new_resolution</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>New resolution in degrees. Default is 0.5.</p> </li> <li> <code>lat_name</code>               (<code>str</code>, default:                   <code>'latitude'</code> )           \u2013            <p>Name of the latitude coordinate. Default is \"latitude\".</p> </li> <li> <code>lon_name</code>               (<code>str</code>, default:                   <code>'longitude'</code> )           \u2013            <p>Name of the longitude coordinate. Default is \"longitude\".</p> </li> <li> <code>agg_funcs</code>               (<code>Dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Aggregation functions for each variable. If None, default aggregation (i.e. mean) is used. Default is None.</p> </li> <li> <code>agg_map</code>               (<code>Dict[str, Callable[[Any], float]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Mapping of string to aggregation functions. If None, default mapping is used. Default is None.</p> </li> <li> <code>expected_longitude_max</code>               (<code>float64</code>, default:                   <code>float64(179.75)</code> )           \u2013            <p>Expected maximum longitude after adjustment. Default is np.float64(179.75).</p> </li> <li> <code>method_map</code>               (<code>Dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Mapping of variable names to interpolation methods. If None, linear interpolation is used. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Resampled dataset with changed resolution.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.shift_time","title":"shift_time","text":"<pre><code>shift_time(dataset, offset=-1, time_unit='D', var_name='time')\n</code></pre> <p>Shift the time coordinate of a dataset by a specified timedelta. The dataset is overwritten with the shifted time values.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset to shift.</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Amount to shift the time coordinate. Default is -1.</p> </li> <li> <code>time_unit</code>               (<code>Literal['W', 'D', 'h', 'm', 's', 'ms', 'ns']</code>, default:                   <code>'D'</code> )           \u2013            <p>Time unit for the shift. Default is \"D\".</p> </li> <li> <code>var_name</code>               (<code>str</code>, default:                   <code>'time'</code> )           \u2013            <p>Name of the time variable in the dataset. Default is \"time\".</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.truncate_data_by_time","title":"truncate_data_by_time","text":"<pre><code>truncate_data_by_time(dataset, start_date, end_date=None, var_name='time')\n</code></pre> <p>Truncate data from a specific start date to an end date. Both dates are inclusive.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset to truncate.</p> </li> <li> <code>start_date</code>               (<code>Union[str, datetime64]</code>)           \u2013            <p>Start date for truncation. Format as \"YYYY-MM-DD\" or as a numpy datetime64 object.</p> </li> <li> <code>end_date</code>               (<code>Union[str, datetime64, None]</code>, default:                   <code>None</code> )           \u2013            <p>End date for truncation. Format as \"YYYY-MM-DD\" or as a numpy datetime64 object. If None, truncate until the last date in the dataset. Default is None.</p> </li> <li> <code>var_name</code>               (<code>str</code>, default:                   <code>'time'</code> )           \u2013            <p>Name of the time variable in the dataset. Default is \"time\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset truncated from the specified start date.</p> </li> </ul>"},{"location":"reference/preprocess/#heiplanet_data.preprocess.upsample_resolution","title":"upsample_resolution","text":"<pre><code>upsample_resolution(dataset, new_resolution=0.1, lat_name='latitude', lon_name='longitude', method_map=None)\n</code></pre> <p>Upsample the resolution of a dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>Dataset to change resolution.</p> </li> <li> <code>new_resolution</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>New resolution in degrees. Default is 0.1.</p> </li> <li> <code>lat_name</code>               (<code>str</code>, default:                   <code>'latitude'</code> )           \u2013            <p>Name of the latitude coordinate. Default is \"latitude\".</p> </li> <li> <code>lon_name</code>               (<code>str</code>, default:                   <code>'longitude'</code> )           \u2013            <p>Name of the longitude coordinate. Default is \"longitude\".</p> </li> <li> <code>method_map</code>               (<code>Dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Mapping of variable names to interpolation methods. If None, linear interpolation is used. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>xr.Dataset: Dataset with changed resolution.</p> </li> </ul>"},{"location":"reference/script/","title":"script","text":""},{"location":"reference/script/#heiplanet_datascript-module","title":"heiplanet_data.script module","text":""},{"location":"reference/script/#heiplanet_data.script","title":"heiplanet_data.script","text":"<p>Attributes:</p> <ul> <li> <code>data_folder</code>           \u2013            </li> <li> <code>data_folder_out</code>           \u2013            </li> <li> <code>data_format</code>           \u2013            </li> <li> <code>dataset</code>           \u2013            </li> <li> <code>file_name</code>           \u2013            </li> <li> <code>output_file</code>           \u2013            </li> <li> <code>popu_file</code>           \u2013            </li> <li> <code>preprocessed_dataset</code>           \u2013            </li> <li> <code>preprocessed_popu</code>           \u2013            </li> <li> <code>request</code>           \u2013            </li> <li> <code>settings</code>           \u2013            </li> </ul>"},{"location":"reference/script/#heiplanet_data.script.data_folder","title":"data_folder  <code>module-attribute</code>","text":"<pre><code>data_folder = Path('.data_heiplanet_db/bronze/')\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.data_folder_out","title":"data_folder_out  <code>module-attribute</code>","text":"<pre><code>data_folder_out = Path('.data_heiplanet_db/silver/')\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.data_format","title":"data_format  <code>module-attribute</code>","text":"<pre><code>data_format = 'netcdf'\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.dataset","title":"dataset  <code>module-attribute</code>","text":"<pre><code>dataset = 'reanalysis-era5-land-monthly-means'\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.file_name","title":"file_name  <code>module-attribute</code>","text":"<pre><code>file_name = get_filename(dataset, data_format, request['year'], request['month'], 'area' in request, 'era5_data', request['variable'])\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.output_file","title":"output_file  <code>module-attribute</code>","text":"<pre><code>output_file = data_folder / file_name\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.popu_file","title":"popu_file  <code>module-attribute</code>","text":"<pre><code>popu_file = data_folder / 'population_histsoc_30arcmin_annual_1901_2021_renamed.nc'\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.preprocessed_dataset","title":"preprocessed_dataset  <code>module-attribute</code>","text":"<pre><code>preprocessed_dataset = preprocess_data_file(netcdf_file=output_file, settings=settings)\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.preprocessed_popu","title":"preprocessed_popu  <code>module-attribute</code>","text":"<pre><code>preprocessed_popu = preprocess_data_file(netcdf_file=popu_file, settings=settings)\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.request","title":"request  <code>module-attribute</code>","text":"<pre><code>request = {'product_type': ['monthly_averaged_reanalysis'], 'variable': ['2m_temperature', 'total_precipitation'], 'year': ['2016', '2017'], 'month': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'], 'time': ['00:00'], 'data_format': data_format, 'download_format': 'unarchived'}\n</code></pre>"},{"location":"reference/script/#heiplanet_data.script.settings","title":"settings  <code>module-attribute</code>","text":"<pre><code>settings = get_settings(setting_path='default', new_settings={}, updated_setting_dir=None, save_updated_settings=False)\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#heiplanet_datautils-module","title":"heiplanet_data.utils module","text":""},{"location":"reference/utils/#heiplanet_data.utils","title":"heiplanet_data.utils","text":"<p>Functions:</p> <ul> <li> <code>generate_unique_tag</code>             \u2013              <p>Generate a unique tag based on the current timestamp and hostname.</p> </li> <li> <code>is_non_empty_file</code>             \u2013              <p>Check if a file exists and is not empty.</p> </li> <li> <code>is_valid_settings</code>             \u2013              <p>Check if the settings are valid.</p> </li> <li> <code>load_settings</code>             \u2013              <p>Get the settings for preprocessing steps.</p> </li> <li> <code>save_settings_to_file</code>             \u2013              <p>Save the settings to a file.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>DEFAULT_SETTINGS_FILE</code>           \u2013            </li> <li> <code>pkg</code>           \u2013            </li> </ul>"},{"location":"reference/utils/#heiplanet_data.utils.DEFAULT_SETTINGS_FILE","title":"DEFAULT_SETTINGS_FILE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_SETTINGS_FILE = {'era5': Path(pkg / 'era5_settings.json'), 'isimip': Path(pkg / 'isimip_settings.json')}\n</code></pre>"},{"location":"reference/utils/#heiplanet_data.utils.pkg","title":"pkg  <code>module-attribute</code>","text":"<pre><code>pkg = files('heiplanet_data')\n</code></pre>"},{"location":"reference/utils/#heiplanet_data.utils.generate_unique_tag","title":"generate_unique_tag","text":"<pre><code>generate_unique_tag()\n</code></pre> <p>Generate a unique tag based on the current timestamp and hostname.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A unique tag in the format \"YYYYMMDD-HHMMSS_hostname\".</p> </li> </ul>"},{"location":"reference/utils/#heiplanet_data.utils.is_non_empty_file","title":"is_non_empty_file","text":"<pre><code>is_non_empty_file(file_path)\n</code></pre> <p>Check if a file exists and is not empty.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>The path to the file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the file exists and is not empty, False otherwise.</p> </li> </ul>"},{"location":"reference/utils/#heiplanet_data.utils.is_valid_settings","title":"is_valid_settings","text":"<pre><code>is_valid_settings(settings)\n</code></pre> <p>Check if the settings are valid. Args:     settings (dict): The settings.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the settings are valid, False otherwise.</p> </li> </ul>"},{"location":"reference/utils/#heiplanet_data.utils.load_settings","title":"load_settings","text":"<pre><code>load_settings(source='era5', setting_path='default', new_settings=None)\n</code></pre> <p>Get the settings for preprocessing steps. If the setting path is \"default\", return the default settings of the source. If the setting path is not default, read the settings from the file. If the new settings are provided, overwrite the default/loaded settings.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>str</code>, default:                   <code>'era5'</code> )           \u2013            <p>Source of the data to get corresponding settings.</p> </li> <li> <code>setting_path</code>               (<code>Path | str</code>, default:                   <code>'default'</code> )           \u2013            <p>Path to the settings file. Defaults to \"default\".</p> </li> <li> <code>new_settings</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>New settings to overwrite the existing settings. Defaults to {}.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[Dict[str, Any], str]</code>           \u2013            <p>Tuple[Dict[str, Any], str]: A tuple containing the settings dictionary and the name of the settings file.</p> </li> </ul>"},{"location":"reference/utils/#heiplanet_data.utils.save_settings_to_file","title":"save_settings_to_file","text":"<pre><code>save_settings_to_file(settings, dir_path=None, file_name='updated_settings.json')\n</code></pre> <p>Save the settings to a file. If dir_path is None, save to the current directory.</p> <p>Parameters:</p> <ul> <li> <code>settings</code>               (<code>dict</code>)           \u2013            <p>The settings.</p> </li> <li> <code>dir_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The path to save the settings file. Defaults to None.</p> </li> <li> <code>file_name</code>               (<code>str</code>, default:                   <code>'updated_settings.json'</code> )           \u2013            <p>The name for the settings file. Defaults to \"updated_settings.json\".</p> </li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/","title":"Tutorial A","text":"In\u00a0[\u00a0]: Copied! <pre># if running on google colab\n# flake8-noqa-cell\n\nif \"google.colab\" in str(get_ipython()):\n    # install packages\n    %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq\n</pre> # if running on google colab # flake8-noqa-cell  if \"google.colab\" in str(get_ipython()):     # install packages     %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq In\u00a0[\u00a0]: Copied! <pre># Import required libraries\nfrom heiplanet_data import inout  # Our custom module for data I/O operations\nfrom pathlib import Path  # For cross-platform file path handling\nfrom matplotlib import pyplot as plt  # For creating plots and visualizations\nimport xarray as xr  # For working with labeled multi-dimensional arrays\nfrom isimip_client.client import ISIMIPClient  # For downloading ISIMIP data\n</pre> # Import required libraries from heiplanet_data import inout  # Our custom module for data I/O operations from pathlib import Path  # For cross-platform file path handling from matplotlib import pyplot as plt  # For creating plots and visualizations import xarray as xr  # For working with labeled multi-dimensional arrays from isimip_client.client import ISIMIPClient  # For downloading ISIMIP data <p>We now need to set up a folder structure where to save the downloaded data.</p> In\u00a0[\u00a0]: Copied! <pre># Set up data directories\n# We use pathlib.Path for cross-platform compatibility\ndata_root = Path(\"data/\")  # Navigate to the data directory from docs/source/notebooks/\ndata_folder = data_root / \"in\"  # Raw input data goes in the 'in' subfolder\n\nprint(f\"Data root directory: {data_root.absolute()}\")\nprint(f\"Input data directory: {data_folder.absolute()}\")\nprint(f\"Directory exists: {data_folder.exists()}\")\n</pre> # Set up data directories # We use pathlib.Path for cross-platform compatibility data_root = Path(\"data/\")  # Navigate to the data directory from docs/source/notebooks/ data_folder = data_root / \"in\"  # Raw input data goes in the 'in' subfolder  print(f\"Data root directory: {data_root.absolute()}\") print(f\"Input data directory: {data_folder.absolute()}\") print(f\"Directory exists: {data_folder.exists()}\") <p>ERA5-Land is a reanalysis dataset providing a comprehensive record of land variables from 1950 to present. It's produced by the European Centre for Medium-Range Weather Forecasts (ECMWF).</p> <p>The CDS's ERA5-Land monthly dataset is being used for now. Please set up the CDS API as outlined below and take note of the naming convention used for the downloaded files.</p> In\u00a0[\u00a0]: Copied! <pre># replace dataset and request with your own values\ndataset = \"reanalysis-era5-land-monthly-means\"\nrequest = {\n    \"product_type\": [\"monthly_averaged_reanalysis\"],\n    \"variable\": [\"2m_temperature\", \"total_precipitation\"],\n    \"year\": [\"2016\", \"2017\"],\n    \"month\": [\n        \"01\",\n        \"02\",\n        \"03\",\n        \"04\",\n        \"05\",\n        \"06\",\n        \"07\",\n        \"08\",\n        \"09\",\n        \"10\",\n        \"11\",\n        \"12\",\n    ],\n    \"time\": [\"00:00\"],\n    \"data_format\": \"netcdf\",\n    \"download_format\": \"unarchived\",\n}\n</pre> # replace dataset and request with your own values dataset = \"reanalysis-era5-land-monthly-means\" request = {     \"product_type\": [\"monthly_averaged_reanalysis\"],     \"variable\": [\"2m_temperature\", \"total_precipitation\"],     \"year\": [\"2016\", \"2017\"],     \"month\": [         \"01\",         \"02\",         \"03\",         \"04\",         \"05\",         \"06\",         \"07\",         \"08\",         \"09\",         \"10\",         \"11\",         \"12\",     ],     \"time\": [\"00:00\"],     \"data_format\": \"netcdf\",     \"download_format\": \"unarchived\", } In\u00a0[\u00a0]: Copied! <pre># Generate a descriptive filename for our downloaded data\ndata_format = request.get(\"data_format\")\n\n# The inout.get_filename() function creates standardized filenames\n# that include key metadata about the dataset\nera5_fname = inout.get_filename(\n    ds_name=dataset,  # Dataset identifier\n    data_format=data_format,  # File format (netcdf)\n    years=request[\"year\"],  # Years included (2016-2017)\n    months=request[\"month\"],  # Months included (all 12)\n    has_area=bool(\"area\" in request),  # Whether spatial subsetting was used\n    base_name=\"era5_data\",  # Base prefix for the filename\n    variables=request[\"variable\"],  # Variables included (t2m, tp)\n)\n\n# Create the full file path\nera5_fpath = data_folder / era5_fname\n\nprint(f\"Generated filename: {era5_fname}\")\nprint(f\"Full file path: {era5_fpath}\")\n</pre> # Generate a descriptive filename for our downloaded data data_format = request.get(\"data_format\")  # The inout.get_filename() function creates standardized filenames # that include key metadata about the dataset era5_fname = inout.get_filename(     ds_name=dataset,  # Dataset identifier     data_format=data_format,  # File format (netcdf)     years=request[\"year\"],  # Years included (2016-2017)     months=request[\"month\"],  # Months included (all 12)     has_area=bool(\"area\" in request),  # Whether spatial subsetting was used     base_name=\"era5_data\",  # Base prefix for the filename     variables=request[\"variable\"],  # Variables included (t2m, tp) )  # Create the full file path era5_fpath = data_folder / era5_fname  print(f\"Generated filename: {era5_fname}\") print(f\"Full file path: {era5_fpath}\") In\u00a0[\u00a0]: Copied! <pre># Download the ERA5 data (this may take several minutes)\n# We first check if the file already exists to avoid unnecessary downloads\nif not era5_fpath.exists():\n    print(\"Downloading ERA5 data from Copernicus Climate Data Store...\")\n    print(\"This may take several minutes depending on file size and server load...\")\n    # The inout.download_data() function handles the CDS API authentication and download\n    inout.download_data(era5_fpath, dataset, request)\n    print(f\"Download complete! File saved to: {era5_fpath}\")\nelse:\n    print(f\"ERA5 data already exists at {era5_fpath}\")\n    print(\"Skipping download to save time.\")\n</pre> # Download the ERA5 data (this may take several minutes) # We first check if the file already exists to avoid unnecessary downloads if not era5_fpath.exists():     print(\"Downloading ERA5 data from Copernicus Climate Data Store...\")     print(\"This may take several minutes depending on file size and server load...\")     # The inout.download_data() function handles the CDS API authentication and download     inout.download_data(era5_fpath, dataset, request)     print(f\"Download complete! File saved to: {era5_fpath}\") else:     print(f\"ERA5 data already exists at {era5_fpath}\")     print(\"Skipping download to save time.\") <p>Some models may require total precipitation data downloaded from the dataset <code>ERA5-Land hourly data from 1950 to present</code>.</p> <p>Due to the nature of this dataset, the value at <code>00:00</code> is the total precipitation of the previous day (see here).</p> <p>To get the correct precipitation values from <code>01.01.2016</code> to <code>31.12.2017</code>, we need to download the data from <code>02.01.2016</code> to <code>01.01.2018</code>. The current CDS request API does not support specifying a continuous date range that does not share the same days for each month and the same months for each year.</p> <p>We implemented a special function for this case.</p> <pre>def download_total_precipitation_from_hourly_era5_land(\n    start_date: str,\n    end_date: str,\n    area: List[float] | None = None,\n    out_dir: Path = Path(\".\"),\n    base_name: str = \"era5_data\",\n    data_format: str = \"netcdf\",\n    ds_name: str = \"reanalysis-era5-land\",\n    coord_name: str = \"valid_time\",\n    var_name: str = \"total_precipitation\",\n    clean_tmp_files: bool = False,\n) -&gt; str:\n</pre> <p>Input for this function includes:</p> <ul> <li><code>start_date</code> and <code>end_date</code> in the format of \"YYYY-MM-DD\"</li> <li><code>area</code> indicates the area to download; <code>None</code> means the entire globe.</li> <li><code>out_dir</code>: output directory to store the downloaded file</li> <li><code>base_name</code>: base string used to name the output file. File name is described in Naming convention - Special case</li> <li><code>data_format</code>: can be <code>netcdf</code> or <code>grib</code></li> <li><code>ds_name</code>, <code>coord_name</code>, and <code>var_name</code> represent the dataset name, coordinate name, and data variable name in the dataset. Please only change these values when CDS changes the corresponding names.</li> <li><code>clean_tmp_files</code> parameter can be set to <code>False</code> to retain the downloaded temporary files, which store data for smaller sub-ranges derived from the overall date range. For example, the range <code>2016-01-01</code> to <code>2017-12-31</code> would be split into sub-ranges <code>2016-01-02</code> to <code>2016-12-31</code>, <code>2017-01-01</code> to <code>2017-12-31</code>, and <code>2018-01-01</code> to <code>2018-01-01</code>, because the timestamps are shifted one day forward.</li> </ul> <p>The function handles time shifting, downloads the data, adjusts the time coordinate back to the target range, and returns the output file path.</p> In\u00a0[\u00a0]: Copied! <pre># download total precipitation data from ERA5-Land Hourly dataset\n# from 2016-01-01 to 2017-12-31\nstart_time = \"2016-01-01\"\nend_time = \"2017-12-31\"\ntp_era5_hourly_file = inout.download_total_precipitation_from_hourly_era5_land(\n    start_date=start_time,\n    end_date=end_time,\n    area=None,\n    out_dir=data_folder,\n    base_name=\"era5_data\",\n    data_format=\"netcdf\",\n    ds_name=\"reanalysis-era5-land\",\n    coord_name=\"valid_time\",\n    var_name=\"total_precipitation\",\n    clean_tmp_files=False,  # keep temporary files for checking\n)\ntp_era5_hourly_file\n</pre> # download total precipitation data from ERA5-Land Hourly dataset # from 2016-01-01 to 2017-12-31 start_time = \"2016-01-01\" end_time = \"2017-12-31\" tp_era5_hourly_file = inout.download_total_precipitation_from_hourly_era5_land(     start_date=start_time,     end_date=end_time,     area=None,     out_dir=data_folder,     base_name=\"era5_data\",     data_format=\"netcdf\",     ds_name=\"reanalysis-era5-land\",     coord_name=\"valid_time\",     var_name=\"total_precipitation\",     clean_tmp_files=False,  # keep temporary files for checking ) tp_era5_hourly_file In\u00a0[\u00a0]: Copied! <pre>tp_era5_hourly_ds = xr.open_dataset(tp_era5_hourly_file)\ntp_era5_hourly_ds[\"valid_time\"]\n</pre> tp_era5_hourly_ds = xr.open_dataset(tp_era5_hourly_file) tp_era5_hourly_ds[\"valid_time\"] <p>ISIMIP (Inter-Sectoral Impact Model Intercomparison Project) provides a framework for comparing and improving impact models across different sectors.</p> In\u00a0[\u00a0]: Copied! <pre># initialize ISIMIP client\nclient = ISIMIPClient()\n</pre> # initialize ISIMIP client client = ISIMIPClient() In\u00a0[\u00a0]: Copied! <pre># search for population data\nresponse = client.datasets(\n    path=\"ISIMIP3a/InputData/socioeconomic/pop/histsoc/population\"\n)  # this path is similar to the one in ISIMIP's website\n\nfor dataset in response[\"results\"]:\n    print(\"Dataset found: {}\".format(dataset[\"path\"]))\n\n# download population data file, 1901_2021\nfor dataset in response[\"results\"]:\n    for file in dataset[\"files\"]:\n        if \"1901_2021\" in file[\"name\"]:\n            isimip_fpath = data_folder / file[\"name\"]\n            if isimip_fpath.exists():\n                print(f\"Population data file already exists: {file['name']}\")\n            else:\n                print(f\"Downloading population data file: {file['name']}\")\n                client.download(file[\"file_url\"], path=data_folder)\n                print(\"Download complete!\")\n            break  # exit after first match\n</pre> # search for population data response = client.datasets(     path=\"ISIMIP3a/InputData/socioeconomic/pop/histsoc/population\" )  # this path is similar to the one in ISIMIP's website  for dataset in response[\"results\"]:     print(\"Dataset found: {}\".format(dataset[\"path\"]))  # download population data file, 1901_2021 for dataset in response[\"results\"]:     for file in dataset[\"files\"]:         if \"1901_2021\" in file[\"name\"]:             isimip_fpath = data_folder / file[\"name\"]             if isimip_fpath.exists():                 print(f\"Population data file already exists: {file['name']}\")             else:                 print(f\"Downloading population data file: {file['name']}\")                 client.download(file[\"file_url\"], path=data_folder)                 print(\"Download complete!\")             break  # exit after first match In\u00a0[\u00a0]: Copied! <pre># load netCDF files\nds_era5 = xr.open_dataset(era5_fpath)\nds_isimip = xr.open_dataset(isimip_fpath)\n</pre> # load netCDF files ds_era5 = xr.open_dataset(era5_fpath) ds_isimip = xr.open_dataset(isimip_fpath) In\u00a0[\u00a0]: Copied! <pre># Examine the ERA5 dataset structure\nprint(\"=== ERA5 DATASET OVERVIEW ===\")\nprint(ds_era5)\nprint(\"\\n\" + \"=\" * 50)\n</pre> # Examine the ERA5 dataset structure print(\"=== ERA5 DATASET OVERVIEW ===\") print(ds_era5) print(\"\\n\" + \"=\" * 50) In\u00a0[\u00a0]: Copied! <pre># Explore specific data variables in the ERA5 dataset\nprint(\"=== ERA5 DATA VARIABLES ===\")\nfor var_name, var in ds_era5.data_vars.items():\n    print(f\"\\n\ud83d\udcca Variable: {var_name}\")\n    print(f\"   Shape: {var.shape}\")\n    print(f\"   Dimensions: {var.dims}\")\n    print(f\"   Data type: {var.dtype}\")\n    if hasattr(var, \"long_name\"):\n        print(f\"   Description: {var.long_name}\")\n    if hasattr(var, \"units\"):\n        print(f\"   Units: {var.units}\")\n</pre> # Explore specific data variables in the ERA5 dataset print(\"=== ERA5 DATA VARIABLES ===\") for var_name, var in ds_era5.data_vars.items():     print(f\"\\n\ud83d\udcca Variable: {var_name}\")     print(f\"   Shape: {var.shape}\")     print(f\"   Dimensions: {var.dims}\")     print(f\"   Data type: {var.dtype}\")     if hasattr(var, \"long_name\"):         print(f\"   Description: {var.long_name}\")     if hasattr(var, \"units\"):         print(f\"   Units: {var.units}\") In\u00a0[\u00a0]: Copied! <pre># Examine coordinates in the ERA5 dataset\nprint(\"=== ERA5 COORDINATES ===\")\nfor coord_name, coord in ds_era5.coords.items():\n    print(f\"\\n\ud83c\udf10 Coordinate: {coord_name}\")\n    print(f\"   Shape: {coord.shape}\")\n    print(f\"   Range: {coord.min().values} to {coord.max().values}\")\n    if coord_name == \"valid_time\":\n        print(f\"   First date: {coord.values[0]}\")\n        print(f\"   Last date: {coord.values[-1]}\")\n        print(f\"   Total time steps: {len(coord)}\")\n    elif coord_name in [\"latitude\", \"longitude\"]:\n        print(f\"   Resolution: ~{abs(coord[1].values - coord[0].values):.3f} degrees\")\n</pre> # Examine coordinates in the ERA5 dataset print(\"=== ERA5 COORDINATES ===\") for coord_name, coord in ds_era5.coords.items():     print(f\"\\n\ud83c\udf10 Coordinate: {coord_name}\")     print(f\"   Shape: {coord.shape}\")     print(f\"   Range: {coord.min().values} to {coord.max().values}\")     if coord_name == \"valid_time\":         print(f\"   First date: {coord.values[0]}\")         print(f\"   Last date: {coord.values[-1]}\")         print(f\"   Total time steps: {len(coord)}\")     elif coord_name in [\"latitude\", \"longitude\"]:         print(f\"   Resolution: ~{abs(coord[1].values - coord[0].values):.3f} degrees\") In\u00a0[\u00a0]: Copied! <pre># Demonstrate xarray data selection methods\nprint(\"=== XARRAY DATA SELECTION EXAMPLES ===\")\n\n# 1. Select data by coordinate values (not indices!)\nprint(\"\\n1. Select temperature data for January 2016:\")\njan_2016_temp = ds_era5.t2m.sel(valid_time=\"2016-01\")\nprint(f\"   Shape: {jan_2016_temp.shape}\")\nprint(f\"   Selected time: {jan_2016_temp.valid_time.values}\")\n\n# 2. Select a specific geographic location\nprint(\"\\n2. Select data for a specific location (52.5\u00b0N, 13.4\u00b0E):\")\nlocation_data = ds_era5.sel(latitude=52.5, longitude=13.4, method=\"nearest\")\nprint(f\"   Shape: {location_data.t2m.shape}\")\nprint(\n    f\"   Actual coordinates: lat={location_data.latitude.values:.2f}, lon={location_data.longitude.values:.2f}\"\n)\n\n# 3. Select a geographic region\nprint(\"\\n3. Select data for an area:\")\narea_data = ds_era5.sel(latitude=slice(70, 35), longitude=slice(0, 40))\nprint(f\"   Shape: {area_data.t2m.shape}\")\nprint(\n    f\"   Lat range: {area_data.latitude.min().values:.1f} to {area_data.latitude.max().values:.1f}\"\n)\nprint(\n    f\"   Lon range: {area_data.longitude.min().values:.1f} to {area_data.longitude.max().values:.1f}\"\n)\n</pre> # Demonstrate xarray data selection methods print(\"=== XARRAY DATA SELECTION EXAMPLES ===\")  # 1. Select data by coordinate values (not indices!) print(\"\\n1. Select temperature data for January 2016:\") jan_2016_temp = ds_era5.t2m.sel(valid_time=\"2016-01\") print(f\"   Shape: {jan_2016_temp.shape}\") print(f\"   Selected time: {jan_2016_temp.valid_time.values}\")  # 2. Select a specific geographic location print(\"\\n2. Select data for a specific location (52.5\u00b0N, 13.4\u00b0E):\") location_data = ds_era5.sel(latitude=52.5, longitude=13.4, method=\"nearest\") print(f\"   Shape: {location_data.t2m.shape}\") print(     f\"   Actual coordinates: lat={location_data.latitude.values:.2f}, lon={location_data.longitude.values:.2f}\" )  # 3. Select a geographic region print(\"\\n3. Select data for an area:\") area_data = ds_era5.sel(latitude=slice(70, 35), longitude=slice(0, 40)) print(f\"   Shape: {area_data.t2m.shape}\") print(     f\"   Lat range: {area_data.latitude.min().values:.1f} to {area_data.latitude.max().values:.1f}\" ) print(     f\"   Lon range: {area_data.longitude.min().values:.1f} to {area_data.longitude.max().values:.1f}\" ) In\u00a0[\u00a0]: Copied! <pre># Demonstrate common xarray operations\nprint(\"=== COMMON XARRAY OPERATIONS ===\")\n\n# 1. Statistical operations across dimensions\nprint(\"\\n1. Calculate statistics across time dimension:\")\ntemp_mean = ds_era5.t2m.mean(dim=\"valid_time\")\ntemp_std = ds_era5.t2m.std(dim=\"valid_time\")\nprint(f\"   Mean temperature shape: {temp_mean.shape} (time dimension removed)\")\nprint(f\"   Global mean temperature: {temp_mean.mean().values:.2f} K\")\nprint(f\"   Standard deviation shape: {temp_std.shape} (time dimension removed)\")\nprint(f\"   Global standard deviation: {temp_std.mean().values:.2f} K\")\n\n# 2. GroupBy operations (seasonal means)\nprint(\"\\n2. Calculate seasonal means using groupby:\")\nseasonal_temp = ds_era5.t2m.groupby(\"valid_time.season\").mean()\nprint(f\"   Seasonal data shape: {seasonal_temp.shape}\")\nprint(f\"   Seasons available: {seasonal_temp.season.values}\")\n\n# 3. Mathematical operations\nprint(\"\\n3. Convert temperature from Kelvin to Celsius:\")\ntemp_celsius = ds_era5.t2m - 273.15  # Simple arithmetic on the entire array\nprint(\n    f\"   Original range: {ds_era5.t2m.min().values:.1f} to {ds_era5.t2m.max().values:.1f} K\"\n)\nprint(\n    f\"   Converted range: {temp_celsius.min().values:.1f} to {temp_celsius.max().values:.1f} \u00b0C\"\n)\n</pre> # Demonstrate common xarray operations print(\"=== COMMON XARRAY OPERATIONS ===\")  # 1. Statistical operations across dimensions print(\"\\n1. Calculate statistics across time dimension:\") temp_mean = ds_era5.t2m.mean(dim=\"valid_time\") temp_std = ds_era5.t2m.std(dim=\"valid_time\") print(f\"   Mean temperature shape: {temp_mean.shape} (time dimension removed)\") print(f\"   Global mean temperature: {temp_mean.mean().values:.2f} K\") print(f\"   Standard deviation shape: {temp_std.shape} (time dimension removed)\") print(f\"   Global standard deviation: {temp_std.mean().values:.2f} K\")  # 2. GroupBy operations (seasonal means) print(\"\\n2. Calculate seasonal means using groupby:\") seasonal_temp = ds_era5.t2m.groupby(\"valid_time.season\").mean() print(f\"   Seasonal data shape: {seasonal_temp.shape}\") print(f\"   Seasons available: {seasonal_temp.season.values}\")  # 3. Mathematical operations print(\"\\n3. Convert temperature from Kelvin to Celsius:\") temp_celsius = ds_era5.t2m - 273.15  # Simple arithmetic on the entire array print(     f\"   Original range: {ds_era5.t2m.min().values:.1f} to {ds_era5.t2m.max().values:.1f} K\" ) print(     f\"   Converted range: {temp_celsius.min().values:.1f} to {temp_celsius.max().values:.1f} \u00b0C\" ) In\u00a0[\u00a0]: Copied! <pre># plot the cartesian grid data of t2m and tp for 2016-2017, all months\nds_era5.t2m.plot.pcolormesh(\n    col=\"valid_time\", col_wrap=4, cmap=\"coolwarm\", robust=True, figsize=(15, 10)\n)\nplt.savefig(\"era5_2016_2017_plots.png\", dpi=300)\nplt.show()\n</pre> # plot the cartesian grid data of t2m and tp for 2016-2017, all months ds_era5.t2m.plot.pcolormesh(     col=\"valid_time\", col_wrap=4, cmap=\"coolwarm\", robust=True, figsize=(15, 10) ) plt.savefig(\"era5_2016_2017_plots.png\", dpi=300) plt.show() In\u00a0[\u00a0]: Copied! <pre># Plot precipitation data\nds_era5.tp.plot.pcolormesh(\n    col=\"valid_time\",\n    col_wrap=4,\n    cmap=\"Blues\",  # Use a sequential colormap for precipitation\n    robust=True,\n    figsize=(15, 10),\n    add_colorbar=True,\n)\n\nplt.suptitle(\n    \"ERA5 Total Precipitation (mm) - 2016-2017 Monthly Data\", fontsize=16, y=1.02\n)\n\nplt.savefig(\"era5_2016_2017_plots_tp.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"\u2713 Precipitation plot saved as 'era5_2016_2017_plots_tp.png'\")\n</pre> # Plot precipitation data ds_era5.tp.plot.pcolormesh(     col=\"valid_time\",     col_wrap=4,     cmap=\"Blues\",  # Use a sequential colormap for precipitation     robust=True,     figsize=(15, 10),     add_colorbar=True, )  plt.suptitle(     \"ERA5 Total Precipitation (mm) - 2016-2017 Monthly Data\", fontsize=16, y=1.02 )  plt.savefig(\"era5_2016_2017_plots_tp.png\", dpi=300, bbox_inches=\"tight\") plt.show()  print(\"\u2713 Precipitation plot saved as 'era5_2016_2017_plots_tp.png'\") In\u00a0[\u00a0]: Copied! <pre># Plot population data for specific years\nprint(\"Creating population plots...\")\n\n# Select population data for 2016 and 2017\npop_2016_2017 = ds_isimip.sel(time=slice(\"2016\", \"2017\"))\n\n# Create population plots\npop_2016_2017[\"total-population\"].plot.pcolormesh(\n    col=\"time\",\n    col_wrap=2,\n    cmap=\"viridis\",  # Use viridis colormap for population\n    robust=True,\n    figsize=(12, 5),\n    add_colorbar=True,\n)\n\nplt.suptitle(\"Total population\", fontsize=16, y=1.02)\nplt.savefig(\"population_2016_2017_plots.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"\u2713 Population plot saved as 'population_2016_2017_plots.png'\")\n</pre> # Plot population data for specific years print(\"Creating population plots...\")  # Select population data for 2016 and 2017 pop_2016_2017 = ds_isimip.sel(time=slice(\"2016\", \"2017\"))  # Create population plots pop_2016_2017[\"total-population\"].plot.pcolormesh(     col=\"time\",     col_wrap=2,     cmap=\"viridis\",  # Use viridis colormap for population     robust=True,     figsize=(12, 5),     add_colorbar=True, )  plt.suptitle(\"Total population\", fontsize=16, y=1.02) plt.savefig(\"population_2016_2017_plots.png\", dpi=300, bbox_inches=\"tight\") plt.show()  print(\"\u2713 Population plot saved as 'population_2016_2017_plots.png'\")"},{"location":"source/notebooks/tutorial_A_download_data/#tutorial-a-download-data-from-copernicus-and-isimip","title":"Tutorial A: Download Data from Copernicus and ISIMIP\u00b6","text":"<p>heiplanet-data Python package - data download and visualization of the raw data</p> <p>Authors: Scientific Software Center Date: October 2025 Version: 1.0</p>"},{"location":"source/notebooks/tutorial_A_download_data/#overview","title":"Overview\u00b6","text":"<p>This tutorial demonstrates how to download data files through the Copernicus and ISIMIP APIs and inspect the data using <code>xarray</code>. You will learn how to:</p> <ol> <li>Data Sources and APIs: Using Copernicus Climate Data Store (CDS) and ISIMIP APIs for data download</li> <li>Xarray: Working with multi-dimensional scientific datasets</li> <li>Data Visualization: Creating plots to verify data integrity</li> </ol>"},{"location":"source/notebooks/tutorial_A_download_data/#1-downloading-the-data-from-the-resources","title":"1. Downloading the data from the resources\u00b6","text":"<p>Let's start by importing the necessary libraries.</p>"},{"location":"source/notebooks/tutorial_A_download_data/#download-era5-land-data","title":"Download ERA5-Land data\u00b6","text":""},{"location":"source/notebooks/tutorial_A_download_data/#what-is-reanalysis-data","title":"What is reanalysis data?\u00b6","text":"<p>Reanalysis combines:</p> <ul> <li>Observations: From weather stations, satellites, radiosondes, etc.</li> <li>Numerical weather models: Physics-based atmospheric models</li> <li>Data assimilation: Mathematical techniques to optimally combine observations with model forecasts</li> </ul> <p>This creates a spatially and temporally consistent dataset that's invaluable for climate research.</p>"},{"location":"source/notebooks/tutorial_A_download_data/#key-era5-land-variables-well-download","title":"Key ERA5-Land variables we'll download\u00b6","text":"<ul> <li>2m temperature (t2m): Air temperature at 2 meters above ground</li> <li>Total precipitation (tp): Accumulated precipitation</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#set-up-cds-api","title":"Set up CDS API\u00b6","text":"<p>To use CDS API for downloading data, you need to first create an account on CDS to obtain your personal access token.</p> <p>Create a <code>.cdsapirc</code> file containing your personal access token by following this instruction.</p>"},{"location":"source/notebooks/tutorial_A_download_data/#general-api-requests","title":"General API requests\u00b6","text":"<ul> <li>Select the target dataset, e.g. ERA5-Land monthly averaged data from 1950 to present</li> <li>Go to tab <code>Download</code> of the dataset and select the data variables, time range, geographical area, etc. that you want to download</li> <li>At the end of the page, click on <code>Show API request code</code> and take notes of the following information<ul> <li><code>dataset</code>: name of the dataset</li> <li><code>request</code>: a dictionary summarizes your download request</li> </ul> </li> <li>Replace the values of <code>dataset</code> and <code>request</code> in the below cell correspondingly</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#understanding-the-request-parameters","title":"Understanding the request parameters\u00b6","text":"<ul> <li><code>dataset</code>: The specific ERA5-Land dataset identifier from CDS</li> <li><code>product_type</code>: Type of product (monthly averaged reanalysis)</li> <li><code>variable</code>: The climate variables we want (temperature and precipitation)</li> <li><code>year</code> and <code>month</code>: Time range for our data (2016-2017, all months)</li> <li><code>time</code>: Specific time of day (00:00 for monthly averages)</li> <li><code>data_format</code>: Output format (NetCDF is standard for scientific data)</li> <li><code>download_format</code>: Whether to compress files (unarchived = no compression)</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#special-download-for-total-precipitation-data-from-era5-land-hourly-dataset","title":"Special download for total precipitation data from ERA5-Land Hourly dataset\u00b6","text":""},{"location":"source/notebooks/tutorial_A_download_data/#naming-convention","title":"Naming convention\u00b6","text":"<p>The filenames of the downloaded netCDF files follow this structure:</p> <pre>{base_name}_{year_str}_{month_str}_{day_str}_{time_str}_{var_str}_{ds_type}_{area_str}_raw.{ext}\n</pre> <ul> <li><code>base_name</code> is <code>\"era5_data\"</code>,</li> <li>For list of numbers, i.e. years/months/days/times, the rule below is applied<ul> <li>If the values are continuous, the string representation is a concatenate of <code>min</code> and <code>max</code> values, separated by <code>-</code></li> <li>Otherwise, the string is a join of all values, separated by <code>_</code></li> <li>However, if there are more than 5 values, we only keep the first 5 ones and replace the rest by <code>\"_etc\"</code></li> <li>If the values are empty (e.g. no days or times in the download request), their string representation and the corresponding separator (i.e. <code>\"_\"</code>) are omitted from the file name.</li> </ul> </li> <li><code>year_str</code> is the string representation of list of years using the rule above.</li> <li>Similarly for <code>month_str</code>. However, if the download requests all 12 months, <code>month_str</code> would be <code>\"allm\"</code></li> <li><code>day_str</code> and <code>time_str</code> follows the same pattern, assuming that a month has at most 31 days (<code>\"alld\"</code>) and a day has at most 24 hours (<code>\"allt\"</code>).<ul> <li>Special case: if data is downloaded at time <code>00:00</code> per day only, <code>time_str</code> would be <code>\"midnight\"</code> (e.g. precipitation data for P-model)</li> </ul> </li> <li>For <code>var_str</code>, each variable has an abbreviation derived by the first letter of each word in the variable name (e.g. <code>tp</code> for <code>total precipitation</code>).<ul> <li>All abbreviations are then concatenated by <code>_</code></li> <li>If this concatenated string is longer than 30 characters, we only keep the first 2 characters and replace the the rest by <code>\"_etc\"</code></li> </ul> </li> <li>As for <code>ds_type</code>:<ul> <li>If the file was downloaded from a monthly dataset, <code>\"monthly\"</code> is set to <code>ds_type</code>. This means the data is recorded only on the first day of each month.</li> <li>For other datasets, when data is downloaded only at midnight (<code>time_str</code> = <code>\"midnight\"</code>), the ds_type is <code>\"daily\"</code>, meaning one data record for one day of each month.</li> <li><code>ds_type</code> would be an empty string in other cases, i.e. multiple data records for each day of a month.</li> </ul> </li> <li>For <code>area_str</code>, if the downloaded data is only for an area of the grid (instead of the whole map), <code>\"area\"</code> would represent for <code>area_str</code>.</li> <li>If the part before <code>\"_raw\"</code> is longer than 100 characters, only the first 100 characters are kept and the rest is replaced by <code>\"_etc\"</code></li> <li><code>\"_raw\"</code> is added at the end to indicate that the file is raw data</li> <li>Extension <code>ext</code> of the file can be <code>.nc</code> or <code>.grib</code></li> <li>If any of these fields (from <code>year_str</code> to <code>area_str</code>) are missing from the download request, the corresponding string and the preceding <code>_</code> are removed from the file name.</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#special-case","title":"Special case\u00b6","text":"<p>As for total precipitation data downloaded from dataset <code>ERA5-Land hourly data from 1950 to present</code>, the file name is structured as:</p> <pre>{base_name}_{start_date}-{end_date}_{time_str}_{var_str}_{ds_type}_{area_str}_raw.{ext}\n</pre> <p>In this case, <code>time_str</code> is <code>\"midnight\"</code> and <code>ds_type</code> is <code>\"daily\"</code>.</p>"},{"location":"source/notebooks/tutorial_A_download_data/#download-isimip-data-population-data","title":"Download ISIMIP data (population data)\u00b6","text":""},{"location":"source/notebooks/tutorial_A_download_data/#about-population-data","title":"About population data\u00b6","text":"<ul> <li>Source: Historical population data from 1901-2021</li> <li>Resolution: 30 arc-minutes (approximately 50km at the equator)</li> <li>Format: NetCDF with population counts per grid cell</li> <li>Units: Number of people per grid cell</li> <li>Use case: Essential for calculating population exposure to climate hazards</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#download-isimip-data-manually","title":"Download ISIMIP data manually\u00b6","text":"<p>To download population data manually, please perform the following steps:</p> <ul> <li>go to ISIMIP website</li> <li>search <code>population</code> from the search bar</li> <li>choose simulation round <code>ISIMIP3a</code></li> <li>click <code>Input Data</code> -&gt; <code>Direct human forcing</code> -&gt; <code>Population data</code> -&gt; <code>histsoc</code></li> <li>choose <code>population_histsoc_30arcmin_annual</code></li> <li>download file <code>population_histsoc_30arcmin_annual_1901_2021.nc</code></li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#download-isimip-data-using-isimips-api","title":"Download ISIMIP data using ISIMIP's API\u00b6","text":""},{"location":"source/notebooks/tutorial_A_download_data/#2-inspect-the-data-using-xarray","title":"2. Inspect the data using <code>xarray</code>\u00b6","text":"<p>Xarray is a powerful Python library for working with labeled multi-dimensional arrays. It's particularly well-suited for scientific datasets like climate data because it:</p> <ul> <li>Labels dimensions: Instead of working with raw indices, you can use meaningful names like 'time', 'latitude', 'longitude'</li> <li>Handles metadata: Stores attributes, coordinate information, and units alongside your data</li> <li>Integrates with pandas: Provides similar functionality for N-dimensional data as pandas does for 2D data</li> <li>Works with NetCDF: Native support for the NetCDF format commonly used in climate science</li> <li>Enables easy operations: Broadcasting, grouping, resampling, and mathematical operations across dimensions</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#key-xarray-concepts","title":"Key Xarray concepts\u00b6","text":"<ul> <li>Dataset: A dictionary-like container of data variables with shared coordinates</li> <li>DataArray: A labeled N-dimensional array (similar to a pandas Series but for N dimensions)</li> <li>Coordinates: Arrays that provide labels for each dimension</li> <li>Attributes: Metadata stored as key-value pairs</li> </ul>"},{"location":"source/notebooks/tutorial_A_download_data/#explore-the-xarray-dataset-structure","title":"Explore the xarray dataset structure\u00b6","text":"<p>Let's examine our datasets to understand their structure. Xarray provides excellent methods for inspecting data.</p>"},{"location":"source/notebooks/tutorial_A_download_data/#understanding-xarray-data-selection","title":"Understanding xarray data selection\u00b6","text":"<p>Xarray provides powerful methods for selecting and indexing data. Let's explore some common operations:</p>"},{"location":"source/notebooks/tutorial_A_download_data/#plot-the-datasets","title":"Plot the datasets\u00b6","text":""},{"location":"source/notebooks/tutorial_B_preprocess_data/","title":"Tutorial B","text":"In\u00a0[\u00a0]: Copied! <pre># if running on google colab\n# flake8-noqa-cell\n\nif \"google.colab\" in str(get_ipython()):\n    # install packages\n    %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq\n</pre> # if running on google colab # flake8-noqa-cell  if \"google.colab\" in str(get_ipython()):     # install packages     %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq <p>Since we will be downloading some data on-the-fly using <code>pooch</code>, you also need to have this library installed in your environment (i.e. <code>pip install pooch</code>.)</p> In\u00a0[\u00a0]: Copied! <pre>from heiplanet_data import preprocess\nfrom pathlib import Path\nimport json\nimport time\nimport pooch\nimport xarray as xr\nfrom matplotlib import pyplot as plt\n</pre> from heiplanet_data import preprocess from pathlib import Path import json import time import pooch import xarray as xr from matplotlib import pyplot as plt In\u00a0[\u00a0]: Copied! <pre># change to your own data folder, if needed\ndata_root = Path(\"data/\")\ndata_folder = data_root / \"in\"\nera5_fname = \"era5_data_2016-2017_allm_2t_tp_monthly_raw.nc\"\nera5_fpath = data_folder / era5_fname\nisimip_fname = \"population_histsoc_30arcmin_annual_1901_2021.nc\"\nisimip_fpath = data_folder / isimip_fname\n</pre> # change to your own data folder, if needed data_root = Path(\"data/\") data_folder = data_root / \"in\" era5_fname = \"era5_data_2016-2017_allm_2t_tp_monthly_raw.nc\" era5_fpath = data_folder / era5_fname isimip_fname = \"population_histsoc_30arcmin_annual_1901_2021.nc\" isimip_fpath = data_folder / isimip_fname In\u00a0[\u00a0]: Copied! <pre>print(f\"Preprocessing ERA5-Land data: {era5_fpath}\")\nt0 = time.time()\npreprocessed_dataset, era5_pfname = preprocess.preprocess_data_file(\n    netcdf_file=era5_fpath,\n    source=\"era5\",\n    settings=\"default\",\n    new_settings=None,\n    unique_tag=\"tutorial_B\",\n)\nt_preprocess = time.time()\nprint(f\"Preprocessing completed in {t_preprocess - t0:.2f} seconds.\")\nprint(f\"Name of preprocessed file: {era5_pfname}\")\n</pre> print(f\"Preprocessing ERA5-Land data: {era5_fpath}\") t0 = time.time() preprocessed_dataset, era5_pfname = preprocess.preprocess_data_file(     netcdf_file=era5_fpath,     source=\"era5\",     settings=\"default\",     new_settings=None,     unique_tag=\"tutorial_B\", ) t_preprocess = time.time() print(f\"Preprocessing completed in {t_preprocess - t0:.2f} seconds.\") print(f\"Name of preprocessed file: {era5_pfname}\") In\u00a0[\u00a0]: Copied! <pre>print(f\"Preprocessing ISIMIP data: {isimip_fpath}\")\nt0 = time.time()\npreprocessed_popu, isimip_pfname = preprocess.preprocess_data_file(\n    netcdf_file=isimip_fpath,\n    source=\"isimip\",\n    settings=\"default\",\n    new_settings=None,\n    unique_tag=None,\n)\nt_popu = time.time()\nprint(f\"Preprocessing ISIMIP data completed in {t_popu - t0:.2f} seconds.\")\nprint(f\"Name of preprocessed file: {isimip_pfname}\")\n</pre> print(f\"Preprocessing ISIMIP data: {isimip_fpath}\") t0 = time.time() preprocessed_popu, isimip_pfname = preprocess.preprocess_data_file(     netcdf_file=isimip_fpath,     source=\"isimip\",     settings=\"default\",     new_settings=None,     unique_tag=None, ) t_popu = time.time() print(f\"Preprocessing ISIMIP data completed in {t_popu - t0:.2f} seconds.\") print(f\"Name of preprocessed file: {isimip_pfname}\") In\u00a0[\u00a0]: Copied! <pre># Download the custom data\nfilename = \"output_JModel_global.nc\"\nurl = \"https://heibox.uni-heidelberg.de/f/ce8cfdab18da414fb259/?dl=1\"\nfilehash = \"df6f6062b57608a6a05afe51653b1fa51219190c67c2e34924e4719609ce6182\"\ntry:\n    file = pooch.retrieve(\n        url=url,\n        known_hash=filehash,\n        fname=filename,\n        path=data_folder,\n    )\nexcept Exception as e:\n    print(f\"Error fetching data: {e}\")\n    raise RuntimeError(f\"Failed to fetch data from {url}\") from e\nprint(f\"Data fetched and saved to {file}\")\n</pre> # Download the custom data filename = \"output_JModel_global.nc\" url = \"https://heibox.uni-heidelberg.de/f/ce8cfdab18da414fb259/?dl=1\" filehash = \"df6f6062b57608a6a05afe51653b1fa51219190c67c2e34924e4719609ce6182\" try:     file = pooch.retrieve(         url=url,         known_hash=filehash,         fname=filename,         path=data_folder,     ) except Exception as e:     print(f\"Error fetching data: {e}\")     raise RuntimeError(f\"Failed to fetch data from {url}\") from e print(f\"Data fetched and saved to {file}\") In\u00a0[\u00a0]: Copied! <pre>jmodel_fpath = data_folder / \"output_JModel_global.nc\"\nsettings_file_path = data_folder / \"settings_JModel_global.json\"\n</pre> jmodel_fpath = data_folder / \"output_JModel_global.nc\" settings_file_path = data_folder / \"settings_JModel_global.json\" <p>We are providing a <code>settings_file_path</code>, where we can either deposit a settings file that contains the settings to be used (for example, if you rerun the same processes multiple times), or we can also save the current settings as input to that file. Note that the settings are always also stored with your processed data, so that you can reproduce any processing steps.</p> <p>For now, we will define the settings in this notebook and store them in <code>settings_file_path</code>:</p> In\u00a0[\u00a0]: Copied! <pre>settings = {\n    \"output_dir\": \"processed\",\n    \"resample_grid\": True,  # Enable grid resampling to specified resolution\n    \"resample_grid_vname\": [\n        \"latitude\",\n        \"longitude\",\n    ],  # Variable names for lat/lon coordinates\n    \"resample_degree\": 0.25,  # Target grid resolution in degrees\n    \"resample_grid_fname\": \"deg_trim\",  # File suffix after grid resampling\n}\n# write the settings to a json file\njson.dump(settings, open(settings_file_path, \"w\"))\n</pre> settings = {     \"output_dir\": \"processed\",     \"resample_grid\": True,  # Enable grid resampling to specified resolution     \"resample_grid_vname\": [         \"latitude\",         \"longitude\",     ],  # Variable names for lat/lon coordinates     \"resample_degree\": 0.25,  # Target grid resolution in degrees     \"resample_grid_fname\": \"deg_trim\",  # File suffix after grid resampling } # write the settings to a json file json.dump(settings, open(settings_file_path, \"w\")) <p>Now we can preprocess the custom data file, using the settings defined above:</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Preprocessing JModel output data: {jmodel_fpath}\")\nt0 = time.time()\npreprocessed_jmodel, jmodel_pfname = preprocess.preprocess_data_file(\n    netcdf_file=jmodel_fpath,\n    source=\"era5\",\n    settings=settings_file_path,\n    new_settings=None,\n    unique_tag=None,\n)\nt_jmodel = time.time()\nprint(f\"Preprocessing JModel data completed in {t_jmodel - t0:.2f} seconds.\")\nprint(f\"Name of preprocessed file: {jmodel_pfname}\")\n</pre> print(f\"Preprocessing JModel output data: {jmodel_fpath}\") t0 = time.time() preprocessed_jmodel, jmodel_pfname = preprocess.preprocess_data_file(     netcdf_file=jmodel_fpath,     source=\"era5\",     settings=settings_file_path,     new_settings=None,     unique_tag=None, ) t_jmodel = time.time() print(f\"Preprocessing JModel data completed in {t_jmodel - t0:.2f} seconds.\") print(f\"Name of preprocessed file: {jmodel_pfname}\") <p>Alternatively, you can provide the settings as a dictionary to <code>new_settings</code>, overwriting the <code>era5</code> default settings on the go.</p> In\u00a0[\u00a0]: Copied! <pre>era5_ds_processed = xr.open_dataset(Path.cwd() / \"processed\" / era5_pfname)\nisimip_ds_processed = xr.open_dataset(Path.cwd() / \"processed\" / isimip_pfname)\njmodel_ds_processed = xr.open_dataset(Path.cwd() / \"processed\" / jmodel_pfname)\n\n# for a comparison, also open the raw ERA5 data\nera5_ds_raw = xr.open_dataset(era5_fpath)\nisimip_ds_raw = xr.open_dataset(isimip_fpath)\njmodel_ds_raw = xr.open_dataset(jmodel_fpath)\n</pre> era5_ds_processed = xr.open_dataset(Path.cwd() / \"processed\" / era5_pfname) isimip_ds_processed = xr.open_dataset(Path.cwd() / \"processed\" / isimip_pfname) jmodel_ds_processed = xr.open_dataset(Path.cwd() / \"processed\" / jmodel_pfname)  # for a comparison, also open the raw ERA5 data era5_ds_raw = xr.open_dataset(era5_fpath) isimip_ds_raw = xr.open_dataset(isimip_fpath) jmodel_ds_raw = xr.open_dataset(jmodel_fpath) <p>Now we can plot the era5 processed and raw data for selected months.</p> In\u00a0[\u00a0]: Copied! <pre># create a plot for raw and processed data next to each other\n# plot the cartesian grid data of t2m for 2016-2 and 2016-8\n\nselected_times = [\n    \"2016-01-01\",\n    \"2016-07-01\",\n]\n\n# Create figure with subplots for temperature comparison\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(\"ERA5 Temperature Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)\n\n# Plot raw data\nfor i, time_val in enumerate(selected_times):\n    raw_data = era5_ds_raw.sel(valid_time=time_val, method=\"nearest\")\n    im1 = raw_data.t2m.plot.pcolormesh(\n        ax=axes[0, i], cmap=\"coolwarm\", robust=True, add_colorbar=False\n    )\n    axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)\n\n    processed_data = era5_ds_processed.sel(time=time_val, method=\"nearest\")\n    im2 = processed_data.t2m.plot.pcolormesh(\n        ax=axes[1, i], cmap=\"coolwarm\", robust=True, add_colorbar=False\n    )\n    axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)\n\n# Adjust layout with more spacing\nplt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right\nplt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots\n\n# Add separate colorbars for each row\ncbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar1.set_label(\"Raw Temperature (K)\", fontsize=10)\n\ncbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar2.set_label(\"Processed Temperature (\u00b0C)\", fontsize=10)\n\nplt.show()\n</pre> # create a plot for raw and processed data next to each other # plot the cartesian grid data of t2m for 2016-2 and 2016-8  selected_times = [     \"2016-01-01\",     \"2016-07-01\", ]  # Create figure with subplots for temperature comparison fig, axes = plt.subplots(2, 2, figsize=(12, 8)) fig.suptitle(\"ERA5 Temperature Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)  # Plot raw data for i, time_val in enumerate(selected_times):     raw_data = era5_ds_raw.sel(valid_time=time_val, method=\"nearest\")     im1 = raw_data.t2m.plot.pcolormesh(         ax=axes[0, i], cmap=\"coolwarm\", robust=True, add_colorbar=False     )     axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)      processed_data = era5_ds_processed.sel(time=time_val, method=\"nearest\")     im2 = processed_data.t2m.plot.pcolormesh(         ax=axes[1, i], cmap=\"coolwarm\", robust=True, add_colorbar=False     )     axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)  # Adjust layout with more spacing plt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots  # Add separate colorbars for each row cbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar1.set_label(\"Raw Temperature (K)\", fontsize=10)  cbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar2.set_label(\"Processed Temperature (\u00b0C)\", fontsize=10)  plt.show() <p>Note that due to the different scale between K and C, the shading of the plots is slightly different.</p> In\u00a0[\u00a0]: Copied! <pre># the same for the precipitation\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(\n    \"ERA5 Precipitation Data Comparison: Raw vs Processed\", fontsize=16, y=0.94\n)\n\n# Plot raw data\nfor i, time_val in enumerate(selected_times):\n    raw_data = era5_ds_raw.sel(valid_time=time_val, method=\"nearest\")\n    im1 = raw_data.tp.plot.pcolormesh(\n        ax=axes[0, i], cmap=\"Blues\", robust=True, add_colorbar=False\n    )\n    axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)\n\n    processed_data = era5_ds_processed.sel(time=time_val, method=\"nearest\")\n    im2 = processed_data.tp.plot.pcolormesh(\n        ax=axes[1, i], cmap=\"Blues\", robust=True, add_colorbar=False\n    )\n    axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)\n\n# Adjust layout with more spacing\nplt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right\nplt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots\n\n# Add separate colorbars for each row\ncbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar1.set_label(\"Raw precipitation (m)\", fontsize=10)\n\ncbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar2.set_label(\"Processed precipitation (mm)\", fontsize=10)\n\nplt.show()\n</pre> # the same for the precipitation fig, axes = plt.subplots(2, 2, figsize=(12, 8)) fig.suptitle(     \"ERA5 Precipitation Data Comparison: Raw vs Processed\", fontsize=16, y=0.94 )  # Plot raw data for i, time_val in enumerate(selected_times):     raw_data = era5_ds_raw.sel(valid_time=time_val, method=\"nearest\")     im1 = raw_data.tp.plot.pcolormesh(         ax=axes[0, i], cmap=\"Blues\", robust=True, add_colorbar=False     )     axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)      processed_data = era5_ds_processed.sel(time=time_val, method=\"nearest\")     im2 = processed_data.tp.plot.pcolormesh(         ax=axes[1, i], cmap=\"Blues\", robust=True, add_colorbar=False     )     axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)  # Adjust layout with more spacing plt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots  # Add separate colorbars for each row cbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar1.set_label(\"Raw precipitation (m)\", fontsize=10)  cbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar2.set_label(\"Processed precipitation (mm)\", fontsize=10)  plt.show() In\u00a0[\u00a0]: Copied! <pre># the same for the population\nselected_times = [\n    \"2016-01-01\",\n    \"2017-01-01\",\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(\"Population Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)\n\n# Plot raw data\nfor i, time_val in enumerate(selected_times):\n    raw_data = isimip_ds_raw.sel(time=time_val, method=\"nearest\")\n    im1 = raw_data[\"total-population\"].plot.pcolormesh(\n        ax=axes[0, i], cmap=\"viridis\", robust=True, add_colorbar=False\n    )\n    axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)\n\n    processed_data = isimip_ds_processed.sel(time=time_val, method=\"nearest\")\n    im2 = processed_data[\"total-population\"].plot.pcolormesh(\n        ax=axes[1, i], cmap=\"viridis\", robust=True, add_colorbar=False\n    )\n    axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)\n\n# Adjust layout with more spacing\nplt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right\nplt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots\n\n# Add separate colorbars for each row\ncbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar1.set_label(\"Total population\", fontsize=10)\n\ncbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar2.set_label(\"Total population\", fontsize=10)\n\nplt.show()\n</pre> # the same for the population selected_times = [     \"2016-01-01\",     \"2017-01-01\", ]  fig, axes = plt.subplots(2, 2, figsize=(12, 8)) fig.suptitle(\"Population Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)  # Plot raw data for i, time_val in enumerate(selected_times):     raw_data = isimip_ds_raw.sel(time=time_val, method=\"nearest\")     im1 = raw_data[\"total-population\"].plot.pcolormesh(         ax=axes[0, i], cmap=\"viridis\", robust=True, add_colorbar=False     )     axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)      processed_data = isimip_ds_processed.sel(time=time_val, method=\"nearest\")     im2 = processed_data[\"total-population\"].plot.pcolormesh(         ax=axes[1, i], cmap=\"viridis\", robust=True, add_colorbar=False     )     axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)  # Adjust layout with more spacing plt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots  # Add separate colorbars for each row cbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar1.set_label(\"Total population\", fontsize=10)  cbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar2.set_label(\"Total population\", fontsize=10)  plt.show() <p>And as well for the data from the model output. Here, we have changed the grid resolution (in this case, upsampled), but in a more general case this would be downsampled from a higher to a lower resolution, to preserve accuracy.</p> In\u00a0[\u00a0]: Copied! <pre>selected_times = [\n    \"2016-01-01\",\n    \"2016-07-01\",\n]\n\n# Create figure with subplots for model comparison\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(\"Jmodel Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)\n\n# Plot raw data\nfor i, time_val in enumerate(selected_times):\n    raw_data = jmodel_ds_raw.sel(time=time_val, method=\"nearest\")\n    im1 = raw_data.R0.plot.pcolormesh(\n        ax=axes[0, i], cmap=\"coolwarm\", robust=True, add_colorbar=False\n    )\n    axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)\n\n    processed_data = jmodel_ds_processed.sel(time=time_val, method=\"nearest\")\n    im2 = processed_data.R0.plot.pcolormesh(\n        ax=axes[1, i], cmap=\"coolwarm\", robust=True, add_colorbar=False\n    )\n    axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)\n\n# Adjust layout with more spacing\nplt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right\nplt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots\n\n# Add separate colorbars for each row\ncbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar1.set_label(\"Raw Temperature (K)\", fontsize=10)\n\ncbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar2.set_label(\"Processed Temperature (\u00b0C)\", fontsize=10)\n\nplt.show()\n</pre> selected_times = [     \"2016-01-01\",     \"2016-07-01\", ]  # Create figure with subplots for model comparison fig, axes = plt.subplots(2, 2, figsize=(12, 8)) fig.suptitle(\"Jmodel Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)  # Plot raw data for i, time_val in enumerate(selected_times):     raw_data = jmodel_ds_raw.sel(time=time_val, method=\"nearest\")     im1 = raw_data.R0.plot.pcolormesh(         ax=axes[0, i], cmap=\"coolwarm\", robust=True, add_colorbar=False     )     axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)      processed_data = jmodel_ds_processed.sel(time=time_val, method=\"nearest\")     im2 = processed_data.R0.plot.pcolormesh(         ax=axes[1, i], cmap=\"coolwarm\", robust=True, add_colorbar=False     )     axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)  # Adjust layout with more spacing plt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots  # Add separate colorbars for each row cbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar1.set_label(\"Raw Temperature (K)\", fontsize=10)  cbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar2.set_label(\"Processed Temperature (\u00b0C)\", fontsize=10)  plt.show() In\u00a0[\u00a0]: Copied! <pre># Now we want to zoom in to a region to inspect the different gridding\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(\"Jmodel Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)\n\n# Plot raw data\nfor i, time_val in enumerate(selected_times):\n    raw_data = jmodel_ds_raw.sel(time=time_val, method=\"nearest\")\n    im1 = raw_data.R0.plot.pcolormesh(\n        ax=axes[0, i], cmap=\"coolwarm\", robust=True, add_colorbar=False\n    )\n    axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)\n    axes[0, i].set_xlim(-11, 35)  # Set axis limits\n    axes[0, i].set_ylim(35, 60)\n\n    processed_data = jmodel_ds_processed.sel(time=time_val, method=\"nearest\")\n    im2 = processed_data.R0.plot.pcolormesh(\n        ax=axes[1, i], cmap=\"coolwarm\", robust=True, add_colorbar=False\n    )\n    axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)\n    axes[1, i].set_xlim(-11, 35)  # Set axis limits\n    axes[1, i].set_ylim(35, 60)\n\n# Adjust layout with more spacing\nplt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right\nplt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots\n\n# Add separate colorbars for each row\ncbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar1.set_label(\"Raw Temperature (K)\", fontsize=10)\n\ncbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8)\ncbar2.set_label(\"Processed Temperature (\u00b0C)\", fontsize=10)\n\nplt.show()\n</pre> # Now we want to zoom in to a region to inspect the different gridding fig, axes = plt.subplots(2, 2, figsize=(12, 8)) fig.suptitle(\"Jmodel Data Comparison: Raw vs Processed\", fontsize=16, y=0.94)  # Plot raw data for i, time_val in enumerate(selected_times):     raw_data = jmodel_ds_raw.sel(time=time_val, method=\"nearest\")     im1 = raw_data.R0.plot.pcolormesh(         ax=axes[0, i], cmap=\"coolwarm\", robust=True, add_colorbar=False     )     axes[0, i].set_title(f\"Raw - {time_val[:7]}\", pad=15)     axes[0, i].set_xlim(-11, 35)  # Set axis limits     axes[0, i].set_ylim(35, 60)      processed_data = jmodel_ds_processed.sel(time=time_val, method=\"nearest\")     im2 = processed_data.R0.plot.pcolormesh(         ax=axes[1, i], cmap=\"coolwarm\", robust=True, add_colorbar=False     )     axes[1, i].set_title(f\"Processed - {time_val[:7]}\", pad=15)     axes[1, i].set_xlim(-11, 35)  # Set axis limits     axes[1, i].set_ylim(35, 60)  # Adjust layout with more spacing plt.tight_layout(rect=[0, 0.05, 0.98, 0.94])  # Leave space for colorbars on the right plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Add space between subplots  # Add separate colorbars for each row cbar1 = fig.colorbar(im1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar1.set_label(\"Raw Temperature (K)\", fontsize=10)  cbar2 = fig.colorbar(im2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8) cbar2.set_label(\"Processed Temperature (\u00b0C)\", fontsize=10)  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#tutorial-b-process-the-downloaded-data","title":"Tutorial B: Process the downloaded data\u00b6","text":"<p>heiplanet-data Python package - data processing and visualization of the processed data</p> <p>Authors: Scientific Software Center Date: October 2025 Version: 1.0</p>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#overview","title":"Overview\u00b6","text":"<p>This tutorial demonstrates how to process downloaded data files through <code>heiplanet-data</code>. You will learn how to:</p> <ol> <li>Specify the settings for <code>heiplanet-data</code>: Work with settings files to store data transformations</li> <li>Data operations: Carry out different data operations such as resampling of the grid</li> <li>Data Visualization: Create plots to verify the processed data</li> </ol>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#heiplanet-data","title":"heiplanet-data\u00b6","text":"<p>The heiplanet-data package can help you transform your data in an efficient way, and stores the settings alongside your data to ensure reproducibility. The flow of operations is visualized in this flowchart:</p> <p></p> <p>For the different data sources, different operations are carried out:</p> <ol> <li>Copernicus data: Here, the columns of the data are renamed from <code>valid_time</code> to <code>time</code>, the longitude is adjusted to lie between -180 and 180 degrees, and the temperature is converted from <code>K</code> to <code>C</code>. Precipitation is converted from <code>m</code> to <code>mm</code>, and the latitude/longitude grid can be resampled to match, for example, the grid resolution of the ISIMIP data.</li> <li>Population data: Here, the columns of the data are renamed from <code>lat</code> to <code>latitude</code> and <code>long</code> to <code>longitude</code>, further, the range of years with data is truncated to a specified range.</li> </ol> <p>We will look at NUTS data and averaging in the next part of the tutorial, <code>tutorial C</code>. For now, let's start by importing the necessary libraries.</p>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#1-settings-specifications","title":"1. Settings specifications\u00b6","text":"<p>We use <code>preprocess</code> module to perform preprocessing steps, using function named <code>preprocess_data_file()</code>:</p> <pre>def preprocess_data_file(\n    netcdf_file: Path,\n    source: Literal[\"era5\", \"isimip\"] = \"era5\",\n    settings: Path | str = \"default\",\n    new_settings: Dict[str, Any] | None = None,\n    unique_tag: str | None = None,\n) -&gt; Tuple[xr.Dataset, str]:\n</pre> <p>Here, <code>netcdf_file</code> defines the path to the input file, while <code>source</code> indicates whether the <code>.nc</code> file is downloaded from ERA5-Land or ISIMIP as these two sources have different preprocessing steps. This is relevant if you are loading default settings. If you are loading a custom settings file for custom data, as we will see later, you should specify the <code>source</code> as <code>era5</code>.</p> <p>The preprocessing steps are determined using a dictionary (JSON file), providied through the <code>settings</code> parameter. This parameter can either be set to a file path or to the string <code>\"default\"</code>. If a file path is given, the settings will be loaded from that file; if loading fails, the default settings for the corresponding source <code>era5</code> or <code>isimip</code> will be used instead. If <code>\"default\"</code> is specified, the default settings of the relevant source are loaded directly.</p> <p>If only certain fields of the default settings need to be updated, these fields and their values can be supplied as a dictionary via the <code>new_settings</code> parameter.</p> <p>The final settings used for preprocessing are saved to a file in the same directory as the preprocessed <code>.nc</code> file. This output directory is defined in the provided settings file. The <code>unique_tag</code> is appended to both the settings file and the resulting <code>.nc</code> file to link them together. By default, this is a combination of host name and date.</p> <p>The settings keys for the data (pre-)processing are defined as follows - you can consult this table when you adjust the settings for your specific data:</p> keyword type and default value description <code>output_dir</code> string, default: <code>\"data/processed\"</code> Directory where processed data will be saved. <code>adjust_longitude</code> boolean, default: <code>true</code> Whether to adjust longitude values to the range [-180, 180]. <code>adjust_longitude_vname</code> string, default: <code>\"longitude\"</code> Variable name of the longitude values to adjust. <code>adjust_longitude_fname</code> string, default: <code>\"adjlon\"</code> Suffix of file names after adjusting longitude values. <code>convert_kelvin_to_celsius</code> boolean, default: <code>true</code> Whether to convert temperature values from Kelvin to Celsius. <code>convert_kelvin_to_celsius_vname</code> string, default: <code>\"t2m\"</code> Variable name of the temperature values to convert. <code>convert_kelvin_to_celsius_fname</code> string, default: <code>\"celsius\"</code> Suffix of file names after converting temperature values to Celsius. <code>convert_m_to_mm_precipitation</code> boolean, default: <code>true</code> Whether to convert precipitation values from meters to millimeters. <code>convert_m_to_mm_precipitation_vname</code> string, default: <code>\"tp\"</code> Variable name of the precipitation values to convert. <code>convert_m_to_mm_precipitation_fname</code> string, default: <code>\"mm\"</code> Suffix of file names after converting precipitation values to millimeters. <code>resample_grid</code> boolean, default: <code>true</code> Whether to resample the grid to a specified resolution. <code>resample_grid_vname</code> array of strings, default: <code>[\"latitude\", \"longitude\"]</code> Variable names of the latitude and longitude values for resampling. <code>resample_degree</code> number, default: <code>0.5</code> Value of the target grid resolution in degrees. <code>resample_grid_fname</code> string, default: <code>\"deg_trim\"</code> Suffix of file names after resampling the grid. <code>truncate_date</code> boolean, default: <code>true</code> Whether to truncate the time series from a specified date. <code>truncate_date_from</code> string, default: <code>\"2016-01-01\"</code> Date in YYYY-MM-DD to truncate the time series from. <code>truncate_date_to</code> string, default: <code>\"2017-12-31\"</code> Date in YYYY-MM-DD to truncate the time series to. <code>truncate_date_vname</code> string, default: <code>\"time\"</code> Variable name of the time values to truncate. <code>unify_coords</code> boolean, default: <code>true</code> Whether to unify coordinate names in the data file. <code>unify_coords_fname</code> string, default: <code>\"unicoords\"</code> Suffix of file names after unifying coordinate names. <code>uni_coords</code> object, default: <code>{\"lat\": \"latitude\", \"lon\": \"longitude\", \"valid_time\": \"time\"}</code> Mapping of variable names to their new names. <p>The following subsections illustrate how the settings in the preprocessing are applied to ERA5-Land data and ISIMIP data.</p>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#2-data-operations","title":"2. Data operations\u00b6","text":""},{"location":"source/notebooks/tutorial_B_preprocess_data/#preprocess-era5-land-data","title":"Preprocess ERA5-Land data\u00b6","text":"<p>Default settings for <code>.nc</code> files from ERA5-Land are:</p> <pre>{\n    \"output_dir\": \"processed\",          # Directory for saved processed files\n    \"adjust_longitude\": true,                         # Enable longitude adjustment to [-180, 180] range\n    \"adjust_longitude_vname\": \"longitude\",            # Variable name for longitude values\n    \"adjust_longitude_fname\": \"adjlon\",               # File suffix after longitude adjustment\n    \"convert_kelvin_to_celsius\": true,                # Enable temperature conversion from K to \u00b0C\n    \"convert_kelvin_to_celsius_vname\": \"t2m\",         # Variable name for temperature data\n    \"convert_kelvin_to_celsius_fname\": \"celsius\",     # File suffix after temperature conversion\n    \"convert_m_to_mm_precipitation\": true,            # Enable precipitation conversion from m to mm\n    \"convert_m_to_mm_precipitation_vname\": \"tp\",      # Variable name for precipitation data\n    \"convert_m_to_mm_precipitation_fname\": \"mm\",      # File suffix after precipitation conversion\n    \"resample_grid\": true,                            # Enable grid resampling to specified resolution\n    \"resample_grid_vname\": [\"latitude\", \"longitude\"], # Variable names for lat/lon coordinates\n    \"resample_degree\": 0.5,                           # Target grid resolution in degrees\n    \"resample_grid_fname\": \"deg_trim\",                # File suffix after grid resampling\n    \"unify_coords\": true,                             # Enable coordinate name standardization\n    \"unify_coords_fname\": \"unicoords\",                # File suffix after coordinate unification\n    \"uni_coords\": {                                   # Mapping of old to new coordinate names\n        \"valid_time\": \"time\"                          # Rename 'valid_time' to 'time'\n    }\n}\n</pre> <p>The output directory for all preprocessed files and utilized settings files is set to path <code>\"processed\"</code>. This path is relative to the current file. For <code>.nc</code> files downloaded from ERA5-Land, we need to perform the following preprocessing steps:</p> <ul> <li>adjust longitude from range $[0..360]$ to $[-180..180]$</li> <li>convert temperature values from Kelvin to Celsius</li> <li>convert precipitation values from meter to millimeter</li> <li>resample the grid from $0.1\u00b0$ to $0.5\u00b0$</li> <li>rename coordinates to a unified name set, i.e. <code>latitude</code>, <code>longitude</code>, and <code>time</code></li> </ul> <p>We toggle these steps by setting the corresponding field to <code>true</code> or <code>false</code>, e.g. <code>\"unify_coords\": false</code> disables coordinate renaming.</p> <p>Fields end with <code>_vname</code> specify which data variables in a <code>.nc</code> file will be used for the corresponding preprocessing step. While fields with <code>_fname</code> define the suffix to add into the file name after the preprocessing step run successfully. For an overview of file name transformation, see flowchart in heiplanet-data section.</p> <p>Ultimately, the <code>uni_coords</code> dictionary defines the mapping between old and new coordinate names.</p>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#preprocess-population-data","title":"Preprocess population data\u00b6","text":"<p>Default settings for <code>.nc</code> files from ISIMIP are:</p> <pre>{\n    \"output_dir\": \"processed\",    # Directory for saved processed files\n    \"truncate_date\": true,                      # Enable time series truncation to specific date range\n    \"truncate_date_from\": \"2016-01-01\",         # Start date for data truncation (YYYY-MM-DD format)\n    \"truncate_date_to\": \"2017-12-31\",           # End date for data truncation (YYYY-MM-DD format)\n    \"truncate_date_vname\": \"time\",              # Variable name for time dimension\n    \"unify_coords\": true,                       # Enable coordinate name standardization\n    \"unify_coords_fname\": \"unicoords\",          # File suffix after coordinate unification\n    \"uni_coords\": {                             # Mapping of old to new coordinate names\n        \"lat\": \"latitude\",                      # Rename 'lat' to 'latitude'\n        \"lon\": \"longitude\"                      # Rename 'lon' to 'longitude'\n    }\n}\n</pre> <p>The resulting files (data and settings) will be saved into <code>output_dir</code> (relative path to the current file). To preprocess <code>.nc</code> files from ISIMIP, we consider these steps:</p> <ul> <li>truncate data in a specific range, with start and end timepoints included</li> <li>unify coordinate names</li> </ul> <p>The naming convention for fields in these settings follows the same pattern described above for ERA5-Land.</p>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#process-custom-data","title":"Process custom data\u00b6","text":"<p>For custom data, there are no default settings. You should specify the source type as <code>era5</code> when you call the preprocess function, and provide your own custom dictionary with the desired settings. An example is shown here for model predictions, resampling the prediction to a grid of 0.25 degrees resolution from an initial 0.5 degrees resolution.</p> <p>First we need to download the data. The below cell will fail if you do not have <code>pooch</code> installed in your environment, in which case you need to <code>pip install pooch</code>.</p>"},{"location":"source/notebooks/tutorial_B_preprocess_data/#3-creating-plots-of-the-pre-processed-data","title":"3. Creating plots of the (pre-)processed data\u00b6","text":"<p>For this, we again use <code>xarray</code> and read the data into <code>xarray</code> datasets.</p>"},{"location":"source/notebooks/tutorial_C_postprocess_data/","title":"Tutorial C","text":"In\u00a0[\u00a0]: Copied! <pre># if running on google colab\n# flake8-noqa-cell\n\nif \"google.colab\" in str(get_ipython()):\n    # install packages\n    %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq\n</pre> # if running on google colab # flake8-noqa-cell  if \"google.colab\" in str(get_ipython()):     # install packages     %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nimport time\nimport pooch\nfrom heiplanet_data import preprocess\nimport xarray as xr\nfrom matplotlib import pyplot as plt\nimport geopandas as gpd\nimport matplotlib as mpl\n</pre> from pathlib import Path import time import pooch from heiplanet_data import preprocess import xarray as xr from matplotlib import pyplot as plt import geopandas as gpd import matplotlib as mpl In\u00a0[\u00a0]: Copied! <pre># change to your own data folder, if needed\ndata_root = Path(\"data/\")\ndata_folder = data_root / \"in\"\nprocessed_folder = Path.cwd() / \"processed\"\n# replace the below lines with your processed data filenames\nera5_pfname = \"era5_data_2016-2017_allm_2t_tp_monthly_unicoords_adjlon_celsius_mm_05deg_trim_tutorial_B.nc\"\nisimip_pfname = \"population_histsoc_30arcmin_annual_1901_2021_unicoords_2016-2017_ts20251014-092525_hssc08.nc\"\njmodel_fname = \"output_JModel_global_025deg_trim_ts20251014-092703_hssc08.nc\"\n</pre> # change to your own data folder, if needed data_root = Path(\"data/\") data_folder = data_root / \"in\" processed_folder = Path.cwd() / \"processed\" # replace the below lines with your processed data filenames era5_pfname = \"era5_data_2016-2017_allm_2t_tp_monthly_unicoords_adjlon_celsius_mm_05deg_trim_tutorial_B.nc\" isimip_pfname = \"population_histsoc_30arcmin_annual_1901_2021_unicoords_2016-2017_ts20251014-092525_hssc08.nc\" jmodel_fname = \"output_JModel_global_025deg_trim_ts20251014-092703_hssc08.nc\" In\u00a0[\u00a0]: Copied! <pre># download the NUTS shapefile\nfilename = \"NUTS_RG_20M_2024_4326.shp.zip\"\nurl = \"https://heibox.uni-heidelberg.de/f/e95125307161470a8906/?dl=1\"\nfilehash = \"246a5e1a3901380f194879cff5a65bbac86c9c906a7871027a3a918dbe6c7e46\"\n\ntry:\n    file = pooch.retrieve(\n        url=url,\n        known_hash=filehash,\n        fname=filename,\n        path=data_folder,\n    )\nexcept Exception as e:\n    print(f\"Error fetching data: {e}\")\n    raise RuntimeError(f\"Failed to fetch data from {url}\") from e\nprint(f\"Data fetched and saved to {file}\")\n</pre> # download the NUTS shapefile filename = \"NUTS_RG_20M_2024_4326.shp.zip\" url = \"https://heibox.uni-heidelberg.de/f/e95125307161470a8906/?dl=1\" filehash = \"246a5e1a3901380f194879cff5a65bbac86c9c906a7871027a3a918dbe6c7e46\"  try:     file = pooch.retrieve(         url=url,         known_hash=filehash,         fname=filename,         path=data_folder,     ) except Exception as e:     print(f\"Error fetching data: {e}\")     raise RuntimeError(f\"Failed to fetch data from {url}\") from e print(f\"Data fetched and saved to {file}\") In\u00a0[\u00a0]: Copied! <pre># NUTS shapefile\nnuts_file = data_folder / \"NUTS_RG_20M_2024_4326.shp.zip\"\n</pre> # NUTS shapefile nuts_file = data_folder / \"NUTS_RG_20M_2024_4326.shp.zip\" <p>Next, we specify which files will be aggregated. The aggregation is carried out simultaneously which is more efficient.</p> In\u00a0[\u00a0]: Copied! <pre># specify which files will be aggregated by NUTS regions\n# key is name of the dataset, value is a tuple of (file path, aggregation mapping dict.)\nnon_nuts_data = {\n    \"era5\": (processed_folder / era5_pfname, None),\n    \"popu\": (processed_folder / isimip_pfname, None),\n    \"jmodel\": (processed_folder / jmodel_fname, None),\n}\n</pre> # specify which files will be aggregated by NUTS regions # key is name of the dataset, value is a tuple of (file path, aggregation mapping dict.) non_nuts_data = {     \"era5\": (processed_folder / era5_pfname, None),     \"popu\": (processed_folder / isimip_pfname, None),     \"jmodel\": (processed_folder / jmodel_fname, None), } In\u00a0[\u00a0]: Copied! <pre># aggregate data by NUTS regions\nt0 = time.time()\naggregated_file = preprocess.aggregate_data_by_nuts(\n    non_nuts_data, nuts_file, normalize_time=True, output_dir=processed_folder\n)\nt1 = time.time()\nprint(f\"Aggregation completed in {t1 - t0:.2f} seconds\")\n</pre> # aggregate data by NUTS regions t0 = time.time() aggregated_file = preprocess.aggregate_data_by_nuts(     non_nuts_data, nuts_file, normalize_time=True, output_dir=processed_folder ) t1 = time.time() print(f\"Aggregation completed in {t1 - t0:.2f} seconds\") In\u00a0[\u00a0]: Copied! <pre># read the netcdf data into xarray\nnuts_grid = xr.open_dataset(aggregated_file)\n</pre> # read the netcdf data into xarray nuts_grid = xr.open_dataset(aggregated_file) In\u00a0[\u00a0]: Copied! <pre># convert the xarray DataArray to pandas DataFrame\n# to be able to merge with the GeoDataFrame\nnuts_grid = nuts_grid.to_dataframe().reset_index()\nnuts_grid.head(5)\n</pre> # convert the xarray DataArray to pandas DataFrame # to be able to merge with the GeoDataFrame nuts_grid = nuts_grid.to_dataframe().reset_index() nuts_grid.head(5) In\u00a0[\u00a0]: Copied! <pre># read the NUTS shapefile\nnuts_shapefile = data_folder / \"NUTS_RG_20M_2024_4326.shp.zip\"\nNUTS_shapes = gpd.read_file(nuts_shapefile)\n# merge the shapes data with the grid values\ndata_nuts = NUTS_shapes.merge(nuts_grid, on=\"NUTS_ID\")\n</pre> # read the NUTS shapefile nuts_shapefile = data_folder / \"NUTS_RG_20M_2024_4326.shp.zip\" NUTS_shapes = gpd.read_file(nuts_shapefile) # merge the shapes data with the grid values data_nuts = NUTS_shapes.merge(nuts_grid, on=\"NUTS_ID\") In\u00a0[\u00a0]: Copied! <pre># Create plots for selected times\nselected_times = [\"2016-01-01\", \"2016-07-01\"]\n</pre> # Create plots for selected times selected_times = [\"2016-01-01\", \"2016-07-01\"] In\u00a0[\u00a0]: Copied! <pre># Create figure with subplots\nfig, axes = plt.subplots(3, 2, figsize=(14, 12))\nfig.suptitle(\"NUTS aggregated data plots\", fontsize=16, y=0.94)\n\n# Plot raw and processed data for selected times\nfor i, time_val in enumerate(selected_times):\n    # Filter data for current timestamp\n    current_data = data_nuts[data_nuts[\"time\"] == time_val]\n    # Use GeoDataFrame.plot to draw polygons colored by the 't2m' column\n    current_data.plot(\n        column=\"t2m\",\n        ax=axes[0, i],\n        cmap=\"coolwarm\",\n        legend=False,\n        missing_kwds={\"color\": \"lightgrey\"},\n    )\n    axes[0, i].set_title(f\"t2m - {time_val[:7]}\", pad=15)\n    axes[0, i].set_xlim(-10, 35)\n    axes[0, i].set_ylim(34, 72)\n\n    current_data.plot(\n        column=\"tp\",\n        ax=axes[1, i],\n        cmap=\"coolwarm\",\n        legend=False,\n        missing_kwds={\"color\": \"lightgrey\"},\n    )\n    axes[1, i].set_title(f\"precipitation - {time_val[:7]}\", pad=15)\n    axes[1, i].set_xlim(-10, 35)\n    axes[1, i].set_ylim(34, 72)\n\n    current_data.plot(\n        column=\"R0\",\n        ax=axes[2, i],\n        cmap=\"coolwarm\",\n        legend=False,\n        missing_kwds={\"color\": \"lightgrey\"},\n    )\n    axes[2, i].set_title(f\"model output - {time_val[:7]}\", pad=15)\n    axes[2, i].set_xlim(-10, 35)\n    axes[2, i].set_ylim(34, 72)\n\n# Adjust layout with more spacing\nplt.tight_layout(\n    rect=[0, 0.05, 0.85, 0.94]\n)  # Adjusted to leave more space for colorbars\nplt.subplots_adjust(hspace=0.25, wspace=0.15)\n\n# Create colorbars using ScalarMappable so they work with GeoDataFrame plots\n\n\nmappable1 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\nmappable1.set_array(data_nuts[\"t2m\"].values)\ncbar1 = fig.colorbar(\n    mappable1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8\n)\ncbar1.set_label(\"Temperature (\u00b0C)\", fontsize=10)\n\nmappable2 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\nmappable2.set_array(data_nuts[\"tp\"].values)\ncbar2 = fig.colorbar(\n    mappable2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8\n)\ncbar2.set_label(\"precipitation\", fontsize=10)\n\nmappable3 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\nmappable3.set_array(data_nuts[\"R0\"].values)\ncbar3 = fig.colorbar(\n    mappable3, ax=axes[2, :], orientation=\"vertical\", pad=0.02, shrink=0.8\n)\ncbar3.set_label(\"Model output (R0)\", fontsize=10)\n\nplt.show()\n</pre> # Create figure with subplots fig, axes = plt.subplots(3, 2, figsize=(14, 12)) fig.suptitle(\"NUTS aggregated data plots\", fontsize=16, y=0.94)  # Plot raw and processed data for selected times for i, time_val in enumerate(selected_times):     # Filter data for current timestamp     current_data = data_nuts[data_nuts[\"time\"] == time_val]     # Use GeoDataFrame.plot to draw polygons colored by the 't2m' column     current_data.plot(         column=\"t2m\",         ax=axes[0, i],         cmap=\"coolwarm\",         legend=False,         missing_kwds={\"color\": \"lightgrey\"},     )     axes[0, i].set_title(f\"t2m - {time_val[:7]}\", pad=15)     axes[0, i].set_xlim(-10, 35)     axes[0, i].set_ylim(34, 72)      current_data.plot(         column=\"tp\",         ax=axes[1, i],         cmap=\"coolwarm\",         legend=False,         missing_kwds={\"color\": \"lightgrey\"},     )     axes[1, i].set_title(f\"precipitation - {time_val[:7]}\", pad=15)     axes[1, i].set_xlim(-10, 35)     axes[1, i].set_ylim(34, 72)      current_data.plot(         column=\"R0\",         ax=axes[2, i],         cmap=\"coolwarm\",         legend=False,         missing_kwds={\"color\": \"lightgrey\"},     )     axes[2, i].set_title(f\"model output - {time_val[:7]}\", pad=15)     axes[2, i].set_xlim(-10, 35)     axes[2, i].set_ylim(34, 72)  # Adjust layout with more spacing plt.tight_layout(     rect=[0, 0.05, 0.85, 0.94] )  # Adjusted to leave more space for colorbars plt.subplots_adjust(hspace=0.25, wspace=0.15)  # Create colorbars using ScalarMappable so they work with GeoDataFrame plots   mappable1 = mpl.cm.ScalarMappable(cmap=\"coolwarm\") mappable1.set_array(data_nuts[\"t2m\"].values) cbar1 = fig.colorbar(     mappable1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8 ) cbar1.set_label(\"Temperature (\u00b0C)\", fontsize=10)  mappable2 = mpl.cm.ScalarMappable(cmap=\"coolwarm\") mappable2.set_array(data_nuts[\"tp\"].values) cbar2 = fig.colorbar(     mappable2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8 ) cbar2.set_label(\"precipitation\", fontsize=10)  mappable3 = mpl.cm.ScalarMappable(cmap=\"coolwarm\") mappable3.set_array(data_nuts[\"R0\"].values) cbar3 = fig.colorbar(     mappable3, ax=axes[2, :], orientation=\"vertical\", pad=0.02, shrink=0.8 ) cbar3.set_label(\"Model output (R0)\", fontsize=10)  plt.show() In\u00a0[\u00a0]: Copied! <pre># also plot the population data\n# note that population is not available for every time step\nselected_times = [\"2016-01-01\", \"2017-01-01\"]\n\n# Create figure with subplots\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Plot raw and processed data for selected times\nfor i, time_val in enumerate(selected_times):\n    # Filter data for current timestamp\n    current_data_pop = data_nuts[data_nuts[\"time\"] == time_val]\n    current_data_pop.plot(\n        column=\"total-population\",\n        ax=axes[i],\n        cmap=\"coolwarm\",\n        legend=False,\n        missing_kwds={\"color\": \"lightgrey\"},\n    )\n    axes[i].set_title(f\"total population - {time_val[:7]}\", pad=15)\n    axes[i].set_xlim(-10, 35)\n    axes[i].set_ylim(34, 72)\n\n# Create colorbars using ScalarMappable so they work with GeoDataFrame plots\n\nmappable1 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\nmappable1.set_array(data_nuts[\"total-population\"].values)\ncbar1 = fig.colorbar(\n    mappable1, ax=axes[:], orientation=\"vertical\", pad=0.02, shrink=0.8\n)\ncbar1.set_label(\"Total population\", fontsize=10)\n\nplt.show()\n</pre> # also plot the population data # note that population is not available for every time step selected_times = [\"2016-01-01\", \"2017-01-01\"]  # Create figure with subplots fig, axes = plt.subplots(1, 2, figsize=(12, 4))  # Plot raw and processed data for selected times for i, time_val in enumerate(selected_times):     # Filter data for current timestamp     current_data_pop = data_nuts[data_nuts[\"time\"] == time_val]     current_data_pop.plot(         column=\"total-population\",         ax=axes[i],         cmap=\"coolwarm\",         legend=False,         missing_kwds={\"color\": \"lightgrey\"},     )     axes[i].set_title(f\"total population - {time_val[:7]}\", pad=15)     axes[i].set_xlim(-10, 35)     axes[i].set_ylim(34, 72)  # Create colorbars using ScalarMappable so they work with GeoDataFrame plots  mappable1 = mpl.cm.ScalarMappable(cmap=\"coolwarm\") mappable1.set_array(data_nuts[\"total-population\"].values) cbar1 = fig.colorbar(     mappable1, ax=axes[:], orientation=\"vertical\", pad=0.02, shrink=0.8 ) cbar1.set_label(\"Total population\", fontsize=10)  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"source/notebooks/tutorial_C_postprocess_data/#tutorial-c-aggregate-the-data","title":"Tutorial C: Aggregate the data\u00b6","text":"<p>heiplanet-data Python package - data aggregation of the processed data into NUTS or other regions</p> <p>Authors: Scientific Software Center Date: October 2025 Version: 1.0</p>"},{"location":"source/notebooks/tutorial_C_postprocess_data/#overview","title":"Overview\u00b6","text":"<p>This tutorial demonstrates how to postprocess data through <code>heiplanet-data</code>. You will learn how to:</p> <ol> <li>Specify the regional averaging for <code>heiplanet-data</code>: Work with NUTS regions to create aggregated maps</li> <li>Data operations: Aggregate the data on regional grids</li> <li>Data Visualization: Create plots to verify the processed data</li> </ol> <p>We will postprocess the data to aggregate in NUTS regions. This can be NUTS0 down to NUTS3, depending on which values you would like to extract from the aggregated data.</p> <p>We will start by importing the necessary libraries.</p>"},{"location":"source/notebooks/tutorial_C_postprocess_data/#1-nuts-specification","title":"1. NUTS specification\u00b6","text":"<p>Eurostat's NUTS definitions are set here and corresponding shapefiles can be downloaded here.</p> <p>For downloading, please choose:</p> <ul> <li>The latest year from NUTS year,</li> <li>File format: <code>SHP</code>,</li> <li>Geometry type: <code>Polygons (RG)</code>,</li> <li>Scale: <code>20M</code></li> <li>CRS: <code>EPSG: 4326</code></li> </ul> <p>Inside the zip folder, there are five different shapefiles, which are all required to display and extract the NUTS regions data.</p> <pre><code>shape data folder\n|____.shp file: geometry data (e.g. polygons)\n|____.shx file: index for geometry data\n|____.dbf file: attribute data for each NUTS region (e.g NUTS name, NUTS ID)\n|____.prj file: information on CRS\n|____.cpg file: character encoding data\n</code></pre> <p>These NUTS definition files are for Europe only. If a country does not have NUTS level $x \\in [1,3]$, the corresponding data for these levels is excluded from the shapefiles. You do not need to extract the zip folder.</p>"},{"location":"source/notebooks/tutorial_C_postprocess_data/#nuts_id-explanation","title":"NUTS_ID explanation\u00b6","text":"<ul> <li>Structure of <code>NUTS_ID</code>: <code>&lt;country&gt;&lt;level&gt;</code></li> <li><code>country</code>: 2 letters, representing name of a country, e.g. DE</li> <li><code>level</code>: 0 to 3 letters or numbers, signifying the level of the NUTS region</li> </ul> <p>First, we need to tell <code>heiplanet-data</code> where to find the geometrical shapes over which we will aggregate. In principle, any shape folder with similar content structure as the NUTS shapefolder should work.</p>"},{"location":"source/notebooks/tutorial_C_postprocess_data/#2-aggregate-the-data-over-nuts-regions","title":"2. Aggregate the data over NUTS regions\u00b6","text":"<p>To aggregate the data, we now need to pass the NUTS file path and the <code>non_nuts_data</code> dictionary to the function for the processing. Here, the keys represent dataset names (used to form the resulting file name), and the values are tuples containing the file path and the aggregation mapping.</p> <p>By default, the aggregation mapping is set to None, which means the mean function will be applied to all data variables during aggregation.</p> <p>An example of aggregation mapping dictionary is:</p> <pre>{\n    \"t2m\": \"mean\", \n    \"tp\": \"sum\"\n}\n</pre> <p>The resulting file name would be:</p> <p>&lt;NUTS_shapefile_name&gt;agg&lt;nc_dataset_names&gt;_&lt;min_yyyy-mm&gt;-&lt;max_yyyy-mm&gt;.nc</p>"},{"location":"source/notebooks/tutorial_C_postprocess_data/#3-plot-the-aggregated-data","title":"3. Plot the aggregated data\u00b6","text":""}]}