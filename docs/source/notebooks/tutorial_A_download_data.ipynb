{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial A: Download Data from Copernicus and ISIMIP\n",
    "\n",
    "**heiplanet-data Python package - data download and visualization of the raw data**\n",
    "\n",
    "---\n",
    "\n",
    "**Authors:** Scientific Software Center  \n",
    "**Date:** October 2025  \n",
    "**Version:** 1.0\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to download data files through the Copernicus and ISIMIP APIs and inspect the data using `xarray`. You will learn how to:\n",
    "\n",
    "1. **Data Sources and APIs**: Using Copernicus Climate Data Store (CDS) and ISIMIP APIs for data download\n",
    "2. **Xarray**: Working with multi-dimensional scientific datasets\n",
    "3. **Data Visualization**: Creating plots to verify data integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Downloading the data from the resources\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on google colab\n",
    "# flake8-noqa-cell\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # install packages\n",
    "    %pip install git+https://github.com/ssciwr/heiplanet-data.git -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from heiplanet_data import inout  # Our custom module for data I/O operations\n",
    "from pathlib import Path  # For cross-platform file path handling\n",
    "from matplotlib import pyplot as plt  # For creating plots and visualizations\n",
    "import xarray as xr  # For working with labeled multi-dimensional arrays\n",
    "from isimip_client.client import ISIMIPClient  # For downloading ISIMIP data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We now need to set up a folder structure where to save the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directories\n",
    "# We use pathlib.Path for cross-platform compatibility\n",
    "data_root = Path(\n",
    "    \"../../../data/\"\n",
    ")  # Navigate to the data directory from docs/source/notebooks/\n",
    "data_folder = data_root / \"in\"  # Raw input data goes in the 'in' subfolder\n",
    "\n",
    "print(f\"Data root directory: {data_root.absolute()}\")\n",
    "print(f\"Input data directory: {data_folder.absolute()}\")\n",
    "print(f\"Directory exists: {data_folder.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Download ERA5-Land data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "**ERA5-Land** is a reanalysis dataset providing a comprehensive record of land variables from 1950 to present. It's produced by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n",
    "\n",
    "#### What is reanalysis data?\n",
    "Reanalysis combines:\n",
    "- **Observations**: From weather stations, satellites, radiosondes, etc.\n",
    "- **Numerical weather models**: Physics-based atmospheric models\n",
    "- **Data assimilation**: Mathematical techniques to optimally combine observations with model forecasts\n",
    "\n",
    "This creates a spatially and temporally consistent dataset that's invaluable for climate research.\n",
    "\n",
    "#### Key ERA5-Land variables we'll download\n",
    "- **2m temperature (t2m)**: Air temperature at 2 meters above ground\n",
    "- **Total precipitation (tp)**: Accumulated precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The [CDS's ERA5-Land monthly](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land-monthly-means?tab=overview) dataset is being used for now. Please set up the CDS API as outlined below and take note of the naming convention used for the downloaded files.\n",
    "\n",
    "#### Set up CDS API\n",
    "To use [CDS](https://cds.climate.copernicus.eu/) API for downloading data, you need to first create an account on CDS to obtain your personal access token.\n",
    "\n",
    "Create a `.cdsapirc` file containing your personal access token by following [this instruction](https://cds.climate.copernicus.eu/how-to-api).\n",
    "\n",
    "\n",
    "\n",
    "#### General API requests\n",
    "\n",
    "* Select the target dataset, e.g. ERA5-Land monthly averaged data from 1950 to present\n",
    "* Go to tab `Download` of the dataset and select the data variables, time range, geographical area, etc. that you want to download\n",
    "* At the end of the page, click on `Show API request code` and take notes of the following information\n",
    "    * `dataset`: name of the dataset\n",
    "    * `request`: a dictionary summarizes your download request\n",
    "* Replace the values of `dataset` and `request` in the below cell correspondingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace dataset and request with your own values\n",
    "dataset = \"reanalysis-era5-land-monthly-means\"\n",
    "request = {\n",
    "    \"product_type\": [\"monthly_averaged_reanalysis\"],\n",
    "    \"variable\": [\"2m_temperature\", \"total_precipitation\"],\n",
    "    \"year\": [\"2016\", \"2017\"],\n",
    "    \"month\": [\n",
    "        \"01\",\n",
    "        \"02\",\n",
    "        \"03\",\n",
    "        \"04\",\n",
    "        \"05\",\n",
    "        \"06\",\n",
    "        \"07\",\n",
    "        \"08\",\n",
    "        \"09\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "        \"12\",\n",
    "    ],\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Understanding the request parameters\n",
    "\n",
    "- **`dataset`**: The specific ERA5-Land dataset identifier from CDS\n",
    "- **`product_type`**: Type of product (monthly averaged reanalysis)\n",
    "- **`variable`**: The climate variables we want (temperature and precipitation)\n",
    "- **`year`** and **`month`**: Time range for our data (2016-2017, all months)\n",
    "- **`time`**: Specific time of day (00:00 for monthly averages)\n",
    "- **`data_format`**: Output format (NetCDF is standard for scientific data)\n",
    "- **`download_format`**: Whether to compress files (unarchived = no compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a descriptive filename for our downloaded data\n",
    "data_format = request.get(\"data_format\")\n",
    "\n",
    "# The inout.get_filename() function creates standardized filenames\n",
    "# that include key metadata about the dataset\n",
    "era5_fname = inout.get_filename(\n",
    "    ds_name=dataset,  # Dataset identifier\n",
    "    data_format=data_format,  # File format (netcdf)\n",
    "    years=request[\"year\"],  # Years included (2016-2017)\n",
    "    months=request[\"month\"],  # Months included (all 12)\n",
    "    has_area=bool(\"area\" in request),  # Whether spatial subsetting was used\n",
    "    base_name=\"era5_data\",  # Base prefix for the filename\n",
    "    variables=request[\"variable\"],  # Variables included (t2m, tp)\n",
    ")\n",
    "\n",
    "# Create the full file path\n",
    "era5_fpath = data_folder / era5_fname\n",
    "\n",
    "print(f\"Generated filename: {era5_fname}\")\n",
    "print(f\"Full file path: {era5_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ERA5 data (this may take several minutes)\n",
    "# We first check if the file already exists to avoid unnecessary downloads\n",
    "if not era5_fpath.exists():\n",
    "    print(\"Downloading ERA5 data from Copernicus Climate Data Store...\")\n",
    "    print(\"This may take several minutes depending on file size and server load...\")\n",
    "    # The inout.download_data() function handles the CDS API authentication and download\n",
    "    inout.download_data(era5_fpath, dataset, request)\n",
    "    print(f\"Download complete! File saved to: {era5_fpath}\")\n",
    "else:\n",
    "    print(f\"ERA5 data already exists at {era5_fpath}\")\n",
    "    print(\"Skipping download to save time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Special download for total precipitation data from ERA5-Land Hourly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Some models may require total precipitation data downloaded from the dataset `ERA5-Land hourly data from 1950 to present`.\n",
    "\n",
    "Due to the nature of this dataset, the value at `00:00` is the total precipitation of the previous day (see [here](https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790)).\n",
    "\n",
    "To get the correct precipitation values from `01.01.2016` to `31.12.2017`, we need to download the data from `02.01.2016` to `01.01.2018`. The current CDS request API does not support specifying a continuous date range that does not share the same days for each month and the same months for each year.\n",
    "\n",
    "We implemented a special function for this case.\n",
    "\n",
    "```python\n",
    "def download_total_precipitation_from_hourly_era5_land(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    area: List[float] | None = None,\n",
    "    out_dir: Path = Path(\".\"),\n",
    "    base_name: str = \"era5_data\",\n",
    "    data_format: str = \"netcdf\",\n",
    "    ds_name: str = \"reanalysis-era5-land\",\n",
    "    coord_name: str = \"valid_time\",\n",
    "    var_name: str = \"total_precipitation\",\n",
    "    clean_tmp_files: bool = False,\n",
    ") -> str:\n",
    "```\n",
    "\n",
    "Input for this function includes:\n",
    "\n",
    "* `start_date` and `end_date` in the format of \"YYYY-MM-DD\"\n",
    "* `area` indicates the area to download; `None` means the entire globe.\n",
    "* `out_dir`: output directory to store the downloaded file\n",
    "* `base_name`: base string used to name the output file. File name is described in [Naming convention - Special case](../../data.md#special-case)\n",
    "* `data_format`: can be `netcdf` or `grib`\n",
    "* `ds_name`, `coord_name`, and `var_name` represent the dataset name, coordinate name, and data variable name in the dataset. Please only change these values when CDS changes the corresponding names.\n",
    "* `clean_tmp_files` parameter can be set to `False` to retain the downloaded temporary files, which store data for smaller sub-ranges derived from the overall date range. For example, the range `2016-01-01` to `2017-12-31` would be split into sub-ranges `2016-01-02` to `2016-12-31`, `2017-01-01` to `2017-12-31`, and `2018-01-01` to `2018-01-01`, because the timestamps are shifted one day forward.\n",
    "\n",
    "The function handles time shifting, downloads the data, adjusts the time coordinate back to the target range, and returns the output file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download total precipitation data from ERA5-Land Hourly dataset\n",
    "# from 2016-01-01 to 2017-12-31\n",
    "start_time = \"2016-01-01\"\n",
    "end_time = \"2017-12-31\"\n",
    "tp_era5_hourly_file = inout.download_total_precipitation_from_hourly_era5_land(\n",
    "    start_date=start_time,\n",
    "    end_date=end_time,\n",
    "    area=None,\n",
    "    out_dir=data_folder,\n",
    "    base_name=\"era5_data\",\n",
    "    data_format=\"netcdf\",\n",
    "    ds_name=\"reanalysis-era5-land\",\n",
    "    coord_name=\"valid_time\",\n",
    "    var_name=\"total_precipitation\",\n",
    "    clean_tmp_files=False,  # keep temporary files for checking\n",
    ")\n",
    "tp_era5_hourly_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_era5_hourly_ds = xr.open_dataset(tp_era5_hourly_file)\n",
    "tp_era5_hourly_ds[\"valid_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Naming convention\n",
    "The filenames of the downloaded netCDF files follow this structure:\n",
    "```text linenums=\"0\"\n",
    "{base_name}_{year_str}_{month_str}_{day_str}_{time_str}_{var_str}_{ds_type}_{area_str}_raw.{ext}\n",
    "```\n",
    "\n",
    "* `base_name` is `\"era5_data\"`,\n",
    "* For list of numbers, i.e. years/months/days/times, the rule below is applied\n",
    "    * If the values are continuous, the string representation is a concatenate of `min` and `max` values, separated by `-`\n",
    "    * Otherwise, the string is a join of all values, separated by `_`\n",
    "    * However, if there are more than 5 values, we only keep the first 5 ones and replace the rest by `\"_etc\"`\n",
    "    * If the values are empty (e.g. no days or times in the download request), their string representation and the corresponding separator (i.e. `\"_\"`) are omitted from the file name.\n",
    "* `year_str` is the string representation of list of years using the rule above.\n",
    "* Similarly for `month_str`. However, if the download requests all 12 months, `month_str` would be `\"allm\"`\n",
    "* `day_str` and `time_str` follows the same pattern, assuming that a month has at most 31 days (`\"alld\"`) and a day has at most 24 hours (`\"allt\"`).\n",
    "    * Special case: if data is downloaded at time `00:00` per day only, `time_str` would be `\"midnight\"` (e.g. precipitation data for P-model)\n",
    "* For `var_str`, each variable has an abbreviation derived by the first letter of each word in the variable name (e.g. `tp` for `total precipitation`).\n",
    "    * All abbreviations are then concatenated by `_`\n",
    "    * If this concatenated string is longer than 30 characters, we only keep the first 2 characters and replace the the rest by `\"_etc\"`\n",
    "* As for `ds_type`:\n",
    "    * If the file was downloaded from a monthly dataset, `\"monthly\"` is set to `ds_type`. This means the data is recorded only on the first day of each month.\n",
    "    * For other datasets, when data is downloaded only at midnight (`time_str` = `\"midnight\"`), the ds_type is `\"daily\"`, meaning one data record for one day of each month.\n",
    "    * `ds_type` would be an empty string in other cases, i.e. multiple data records for each day of a month.\n",
    "* For `area_str`, if the downloaded data is only for an area of the grid (instead of the whole map), `\"area\"` would represent for `area_str`.\n",
    "* If the part before `\"_raw\"` is longer than 100 characters, only the first 100 characters are kept and the rest is replaced by `\"_etc\"`\n",
    "* `\"_raw\"` is added at the end to indicate that the file is raw data\n",
    "* Extension `ext` of the file can be `.nc` or `.grib`\n",
    "* If any of these fields (from `year_str` to `area_str`) are missing from the download request, the corresponding string and the preceding `_` are removed from the file name.\n",
    "\n",
    "##### Special case\n",
    "\n",
    "As for total precipitation data downloaded from dataset `ERA5-Land hourly data from 1950 to present`, the file name is structured as:\n",
    "\n",
    "```text linenums=\"0\"\n",
    "{base_name}_{start_date}-{end_date}_{time_str}_{var_str}_{ds_type}_{area_str}_raw.{ext}\n",
    "```\n",
    "\n",
    "In this case, `time_str` is `\"midnight\"` and `ds_type` is `\"daily\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Download ISIMIP data (population data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "**ISIMIP** (Inter-Sectoral Impact Model Intercomparison Project) provides a framework for comparing and improving impact models across different sectors.\n",
    "\n",
    "#### About population data\n",
    "- **Source**: Historical population data from 1901-2021\n",
    "- **Resolution**: 30 arc-minutes (approximately 50km at the equator)\n",
    "- **Format**: NetCDF with population counts per grid cell\n",
    "- **Units**: Number of people per grid cell\n",
    "- **Use case**: Essential for calculating population exposure to climate hazards\n",
    "\n",
    "#### Download ISIMIP data manually\n",
    "\n",
    "To download population data manually, please perform the following steps:\n",
    "\n",
    "* go to [ISIMIP website](https://data.isimip.org/)\n",
    "* search `population` from the search bar\n",
    "* choose simulation round `ISIMIP3a`\n",
    "* click `Input Data` -> `Direct human forcing` -> `Population data` -> `histsoc`\n",
    "* choose `population_histsoc_30arcmin_annual`\n",
    "* download file `population_histsoc_30arcmin_annual_1901_2021.nc`\n",
    "\n",
    "#### Download ISIMIP data using ISIMIP's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ISIMIP client\n",
    "client = ISIMIPClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for population data\n",
    "response = client.datasets(\n",
    "    path=\"ISIMIP3a/InputData/socioeconomic/pop/histsoc/population\"\n",
    ")  # this path is similar to the one in ISIMIP's website\n",
    "\n",
    "for dataset in response[\"results\"]:\n",
    "    print(\"Dataset found: {}\".format(dataset[\"path\"]))\n",
    "\n",
    "# download population data file, 1901_2021\n",
    "for dataset in response[\"results\"]:\n",
    "    for file in dataset[\"files\"]:\n",
    "        if \"1901_2021\" in file[\"name\"]:\n",
    "            isimip_fpath = data_folder / file[\"name\"]\n",
    "            if isimip_fpath.exists():\n",
    "                print(f\"Population data file already exists: {file['name']}\")\n",
    "            else:\n",
    "                print(f\"Downloading population data file: {file['name']}\")\n",
    "                client.download(file[\"file_url\"], path=data_folder)\n",
    "                print(\"Download complete!\")\n",
    "            break  # exit after first match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 2. Inspect the data using `xarray`\n",
    "\n",
    "**Xarray** is a powerful Python library for working with labeled multi-dimensional arrays. It's particularly well-suited for scientific datasets like climate data because it:\n",
    "\n",
    "- **Labels dimensions**: Instead of working with raw indices, you can use meaningful names like 'time', 'latitude', 'longitude'\n",
    "- **Handles metadata**: Stores attributes, coordinate information, and units alongside your data\n",
    "- **Integrates with pandas**: Provides similar functionality for N-dimensional data as pandas does for 2D data\n",
    "- **Works with NetCDF**: Native support for the NetCDF format commonly used in climate science\n",
    "- **Enables easy operations**: Broadcasting, grouping, resampling, and mathematical operations across dimensions\n",
    "\n",
    "### Key Xarray concepts\n",
    "- **Dataset**: A dictionary-like container of data variables with shared coordinates\n",
    "- **DataArray**: A labeled N-dimensional array (similar to a pandas Series but for N dimensions)\n",
    "- **Coordinates**: Arrays that provide labels for each dimension\n",
    "- **Attributes**: Metadata stored as key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load netCDF files\n",
    "ds_era5 = xr.open_dataset(era5_fpath)\n",
    "ds_isimip = xr.open_dataset(isimip_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Explore the xarray dataset structure\n",
    "\n",
    "Let's examine our datasets to understand their structure. Xarray provides excellent methods for inspecting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the ERA5 dataset structure\n",
    "print(\"=== ERA5 DATASET OVERVIEW ===\")\n",
    "print(ds_era5)\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore specific data variables in the ERA5 dataset\n",
    "print(\"=== ERA5 DATA VARIABLES ===\")\n",
    "for var_name, var in ds_era5.data_vars.items():\n",
    "    print(f\"\\n📊 Variable: {var_name}\")\n",
    "    print(f\"   Shape: {var.shape}\")\n",
    "    print(f\"   Dimensions: {var.dims}\")\n",
    "    print(f\"   Data type: {var.dtype}\")\n",
    "    if hasattr(var, \"long_name\"):\n",
    "        print(f\"   Description: {var.long_name}\")\n",
    "    if hasattr(var, \"units\"):\n",
    "        print(f\"   Units: {var.units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine coordinates in the ERA5 dataset\n",
    "print(\"=== ERA5 COORDINATES ===\")\n",
    "for coord_name, coord in ds_era5.coords.items():\n",
    "    print(f\"\\n🌐 Coordinate: {coord_name}\")\n",
    "    print(f\"   Shape: {coord.shape}\")\n",
    "    print(f\"   Range: {coord.min().values} to {coord.max().values}\")\n",
    "    if coord_name == \"valid_time\":\n",
    "        print(f\"   First date: {coord.values[0]}\")\n",
    "        print(f\"   Last date: {coord.values[-1]}\")\n",
    "        print(f\"   Total time steps: {len(coord)}\")\n",
    "    elif coord_name in [\"latitude\", \"longitude\"]:\n",
    "        print(f\"   Resolution: ~{abs(coord[1].values - coord[0].values):.3f} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Understanding xarray data selection\n",
    "\n",
    "Xarray provides powerful methods for selecting and indexing data. Let's explore some common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate xarray data selection methods\n",
    "print(\"=== XARRAY DATA SELECTION EXAMPLES ===\")\n",
    "\n",
    "# 1. Select data by coordinate values (not indices!)\n",
    "print(\"\\n1. Select temperature data for January 2016:\")\n",
    "jan_2016_temp = ds_era5.t2m.sel(valid_time=\"2016-01\")\n",
    "print(f\"   Shape: {jan_2016_temp.shape}\")\n",
    "print(f\"   Selected time: {jan_2016_temp.valid_time.values}\")\n",
    "\n",
    "# 2. Select a specific geographic location\n",
    "print(\"\\n2. Select data for a specific location (52.5°N, 13.4°E):\")\n",
    "location_data = ds_era5.sel(latitude=52.5, longitude=13.4, method=\"nearest\")\n",
    "print(f\"   Shape: {location_data.t2m.shape}\")\n",
    "print(\n",
    "    f\"   Actual coordinates: lat={location_data.latitude.values:.2f}, lon={location_data.longitude.values:.2f}\"\n",
    ")\n",
    "\n",
    "# 3. Select a geographic region\n",
    "print(\"\\n3. Select data for an area:\")\n",
    "area_data = ds_era5.sel(latitude=slice(70, 35), longitude=slice(0, 40))\n",
    "print(f\"   Shape: {area_data.t2m.shape}\")\n",
    "print(\n",
    "    f\"   Lat range: {area_data.latitude.min().values:.1f} to {area_data.latitude.max().values:.1f}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Lon range: {area_data.longitude.min().values:.1f} to {area_data.longitude.max().values:.1f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate common xarray operations\n",
    "print(\"=== COMMON XARRAY OPERATIONS ===\")\n",
    "\n",
    "# 1. Statistical operations across dimensions\n",
    "print(\"\\n1. Calculate statistics across time dimension:\")\n",
    "temp_mean = ds_era5.t2m.mean(dim=\"valid_time\")\n",
    "temp_std = ds_era5.t2m.std(dim=\"valid_time\")\n",
    "print(f\"   Mean temperature shape: {temp_mean.shape} (time dimension removed)\")\n",
    "print(f\"   Global mean temperature: {temp_mean.mean().values:.2f} K\")\n",
    "print(f\"   Standard deviation shape: {temp_std.shape} (time dimension removed)\")\n",
    "print(f\"   Global standard deviation: {temp_std.mean().values:.2f} K\")\n",
    "\n",
    "# 2. GroupBy operations (seasonal means)\n",
    "print(\"\\n2. Calculate seasonal means using groupby:\")\n",
    "seasonal_temp = ds_era5.t2m.groupby(\"valid_time.season\").mean()\n",
    "print(f\"   Seasonal data shape: {seasonal_temp.shape}\")\n",
    "print(f\"   Seasons available: {seasonal_temp.season.values}\")\n",
    "\n",
    "# 3. Mathematical operations\n",
    "print(\"\\n3. Convert temperature from Kelvin to Celsius:\")\n",
    "temp_celsius = ds_era5.t2m - 273.15  # Simple arithmetic on the entire array\n",
    "print(\n",
    "    f\"   Original range: {ds_era5.t2m.min().values:.1f} to {ds_era5.t2m.max().values:.1f} K\"\n",
    ")\n",
    "print(\n",
    "    f\"   Converted range: {temp_celsius.min().values:.1f} to {temp_celsius.max().values:.1f} °C\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Plot the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cartesian grid data of t2m and tp for 2016-2017, all months\n",
    "ds_era5.t2m.plot.pcolormesh(\n",
    "    col=\"valid_time\", col_wrap=4, cmap=\"coolwarm\", robust=True, figsize=(15, 10)\n",
    ")\n",
    "plt.savefig(\"era5_2016_2017_plots.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precipitation data\n",
    "ds_era5.tp.plot.pcolormesh(\n",
    "    col=\"valid_time\",\n",
    "    col_wrap=4,\n",
    "    cmap=\"Blues\",  # Use a sequential colormap for precipitation\n",
    "    robust=True,\n",
    "    figsize=(15, 10),\n",
    "    add_colorbar=True,\n",
    ")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"ERA5 Total Precipitation (mm) - 2016-2017 Monthly Data\", fontsize=16, y=1.02\n",
    ")\n",
    "\n",
    "plt.savefig(\"era5_2016_2017_plots_tp.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Precipitation plot saved as 'era5_2016_2017_plots_tp.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot population data for specific years\n",
    "print(\"Creating population plots...\")\n",
    "\n",
    "# Select population data for 2016 and 2017\n",
    "pop_2016_2017 = ds_isimip.sel(time=slice(\"2016\", \"2017\"))\n",
    "\n",
    "# Create population plots\n",
    "pop_2016_2017[\"total-population\"].plot.pcolormesh(\n",
    "    col=\"time\",\n",
    "    col_wrap=2,\n",
    "    cmap=\"viridis\",  # Use viridis colormap for population\n",
    "    robust=True,\n",
    "    figsize=(12, 5),\n",
    "    add_colorbar=True,\n",
    ")\n",
    "\n",
    "plt.suptitle(\"Total population\", fontsize=16, y=1.02)\n",
    "plt.savefig(\"population_2016_2017_plots.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Population plot saved as 'population_2016_2017_plots.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heiplanet-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
