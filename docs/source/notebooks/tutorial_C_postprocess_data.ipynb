{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial C: Aggregate the data\n",
    "\n",
    "**heiplanet-data Python package - data processing and visualization of the processed data**\n",
    "\n",
    "---\n",
    "\n",
    "**Authors:** Scientific Software Center  \n",
    "**Date:** October 2025  \n",
    "**Version:** 1.0\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to postprocess data through `heiplanet-data`. You will learn how to:\n",
    "\n",
    "1. **Specify the regional averaging for `heiplanet-data`**: Work with NUTS regions to create aggregated maps\n",
    "2. **Data operations**: Aggregate the data on regional grids\n",
    "3. **Data Visualization**: Create plots to verify the processed data\n",
    "\n",
    "We will postprocess the data to aggregate in NUTS regions. This can be NUTS0 down to NUTS3, depending on which values you would like to extract from the aggregated data.\n",
    "\n",
    "We will start by importing the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on google colab\n",
    "# flake8-noqa-cell\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # install packages\n",
    "    %pip install git+https://github.com/ssciwr/onehealth-data-backend.git -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import pooch\n",
    "import zipfile\n",
    "from heiplanet_data import preprocess\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to your own data folder, if needed\n",
    "data_root = Path(\"../../../data/\")\n",
    "data_folder = data_root / \"in\"\n",
    "processed_folder = data_root / \"processed\"\n",
    "# replace the below lines with your processed data filenames\n",
    "era5_pfname = \"era5_data_2016-2017_allm_2t_tp_monthly_unicoords_adjlon_celsius_mm_05deg_trim_ts20251008-124654_hssc-laptop01.nc\"\n",
    "isimip_pfname = \"population_histsoc_30arcmin_annual_1901_2021_unicoords_2016-2017_ts20251008-124701_hssc-laptop01.nc\"\n",
    "jmodel_fname = \"output_JModel_global.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NUTS shapefile\n",
    "filename = \"NUTS_RG_20M_2024_4326.shp.zip\"\n",
    "url = \"https://heibox.uni-heidelberg.de/f/e95125307161470a8906/?dl=1\"\n",
    "filehash = \"246a5e1a3901380f194879cff5a65bbac86c9c906a7871027a3a918dbe6c7e46\"\n",
    "\n",
    "try:\n",
    "    file = pooch.retrieve(\n",
    "        url=url,\n",
    "        known_hash=filehash,\n",
    "        fname=filename,\n",
    "        path=data_folder,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    raise RuntimeError(f\"Failed to fetch data from {url}\") from e\n",
    "print(f\"Data fetched and saved to {file}\")\n",
    "\n",
    "# unzip the shapefile\n",
    "with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1. NUTS specification\n",
    "\n",
    "Eurostat's NUTS definitions are set [here](https://ec.europa.eu/eurostat/en/web/products-manuals-and-guidelines/w/ks-gq-23-010) and corresponding shapefiles can be downloaded [here](https://ec.europa.eu/eurostat/web/gisco/geodata/statistical-units/territorial-units-statistics).\n",
    "\n",
    "For downloading, please choose:\n",
    "\n",
    "* The latest year from NUTS year,\n",
    "* File format: `SHP`,\n",
    "* Geometry type: `Polygons (RG)`,\n",
    "* Scale: `20M`\n",
    "* CRS: `EPSG: 4326`\n",
    "\n",
    "Inside the zip folder, there are five different shapefiles, which are all required to display and extract the NUTS regions data.\n",
    "```\n",
    "shape data folder\n",
    "|____.shp file: geometry data (e.g. polygons)\n",
    "|____.shx file: index for geometry data\n",
    "|____.dbf file: attribute data for each NUTS region (e.g NUTS name, NUTS ID)\n",
    "|____.prj file: information on CRS\n",
    "|____.cpg file: character encoding data\n",
    "```\n",
    "These NUTS definition files are for Europe only. If a country does not have NUTS level $x \\in [1,3]$, the corresponding data for these levels is excluded from the shapefiles. You do not need to extract the zip folder.\n",
    "\n",
    "#### `NUTS_ID` explanation:\n",
    "* Structure of `NUTS_ID`: `<country><level>`\n",
    "* `country`: 2 letters, representing name of a country, e.g. DE\n",
    "* `level`: 0 to 3 letters or numbers, signifying the level of the NUTS region\n",
    "\n",
    "First, we need to tell `heiplanet-data` where to find the geometrical shapes over which we will aggregate. In principle, any shape folder with similar content structure as the NUTS shapefolder should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUTS shapefile\n",
    "nuts_file = data_folder / \"NUTS_RG_20M_2024_4326.shp.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Next, we specify which files will be aggregated. The aggregation is carried out simultaneously which is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which files will be aggregated by NUTS regions\n",
    "# key is name of the dataset, value is a tuple of (file path, aggregation mapping dict.)\n",
    "non_nuts_data = {\n",
    "    \"era5\": (processed_folder / era5_pfname, None),\n",
    "    \"popu\": (processed_folder / isimip_pfname, None),\n",
    "    \"jmodel\": (data_folder / jmodel_fname, None),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2. Aggregate the data over NUTS regions\n",
    "To aggregate the data, we now need to pass the NUTS file path and the `non_nuts_data` dictionary to the function for the processing. Here, the keys represent dataset names (used to form the resulting file name), and the values are tuples containing the file path and the aggregation mapping.\n",
    "\n",
    "By default, the aggregation mapping is set to None, which means the mean function will be applied to all data variables during aggregation.\n",
    "\n",
    "An example of aggregation mapping dictionary is:\n",
    "\n",
    "{\n",
    "    \"t2m\": \"mean\", \n",
    "    \"tp\": \"sum\"\n",
    "}\n",
    "The resulting file name would be:\n",
    "\n",
    "<NUTS_shapefile_name>_agg_<nc_dataset_names>_<min_yyyy-mm>-<max_yyyy-mm>.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data by NUTS regions\n",
    "t0 = time.time()\n",
    "aggregated_file = preprocess.aggregate_data_by_nuts(\n",
    "    non_nuts_data, nuts_file, normalize_time=True, output_dir=processed_folder\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"Aggregation completed in {t1 - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 3. Plot the aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the netcdf data into xarray\n",
    "nuts_grid = xr.open_dataset(aggregated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the xarray DataArray to pandas DataFrame\n",
    "# to be able to merge with the GeoDataFrame\n",
    "nuts_grid = nuts_grid.to_dataframe().reset_index()\n",
    "nuts_grid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the NUTS shapefile, for this you do need an extracted shapefile\n",
    "nuts_shapefile = data_folder / \"NUTS_RG_20M_2024_4326.shp\"\n",
    "NUTS_shapes = gpd.read_file(nuts_shapefile)\n",
    "# merge the shapes data with the grid values\n",
    "data_nuts = NUTS_shapes.merge(nuts_grid, on=\"NUTS_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for selected times\n",
    "selected_times = [\"2016-01-01\", \"2017-07-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle(\"NUTS aggregated data plots\", fontsize=16, y=0.94)\n",
    "\n",
    "# Plot raw and processed data for selected times\n",
    "for i, time_val in enumerate(selected_times):\n",
    "    # Filter data for current timestamp\n",
    "    current_data = data_nuts[data_nuts[\"time\"] == time_val]\n",
    "    # Use GeoDataFrame.plot to draw polygons colored by the 't2m' column\n",
    "    current_data.plot(\n",
    "        column=\"t2m\",\n",
    "        ax=axes[0, i],\n",
    "        cmap=\"coolwarm\",\n",
    "        legend=False,\n",
    "        missing_kwds={\"color\": \"lightgrey\"},\n",
    "    )\n",
    "    axes[0, i].set_title(f\"t2m - {time_val[:7]}\", pad=15)\n",
    "    axes[0, i].set_xlim(-10, 35)\n",
    "    axes[0, i].set_ylim(34, 72)\n",
    "\n",
    "    current_data.plot(\n",
    "        column=\"tp\",\n",
    "        ax=axes[1, i],\n",
    "        cmap=\"coolwarm\",\n",
    "        legend=False,\n",
    "        missing_kwds={\"color\": \"lightgrey\"},\n",
    "    )\n",
    "    axes[1, i].set_title(f\"precipitation - {time_val[:7]}\", pad=15)\n",
    "    axes[1, i].set_xlim(-10, 35)\n",
    "    axes[1, i].set_ylim(34, 72)\n",
    "\n",
    "    current_data.plot(\n",
    "        column=\"t2m\",\n",
    "        ax=axes[2, i],\n",
    "        cmap=\"coolwarm\",\n",
    "        legend=False,\n",
    "        missing_kwds={\"color\": \"lightgrey\"},\n",
    "    )\n",
    "    axes[2, i].set_title(f\"model output - {time_val[:7]}\", pad=15)\n",
    "    axes[2, i].set_xlim(-10, 35)\n",
    "    axes[2, i].set_ylim(34, 72)\n",
    "\n",
    "# Adjust layout with more spacing\n",
    "plt.tight_layout(\n",
    "    rect=[0, 0.05, 0.85, 0.94]\n",
    ")  # Adjusted to leave more space for colorbars\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.15)\n",
    "\n",
    "# Create colorbars using ScalarMappable so they work with GeoDataFrame plots\n",
    "\n",
    "\n",
    "mappable1 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\n",
    "mappable1.set_array(data_nuts[\"t2m\"].values)\n",
    "cbar1 = fig.colorbar(\n",
    "    mappable1, ax=axes[0, :], orientation=\"vertical\", pad=0.02, shrink=0.8\n",
    ")\n",
    "cbar1.set_label(\"Temperature (Â°C)\", fontsize=10)\n",
    "\n",
    "mappable2 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\n",
    "mappable2.set_array(data_nuts[\"tp\"].values)\n",
    "cbar2 = fig.colorbar(\n",
    "    mappable2, ax=axes[1, :], orientation=\"vertical\", pad=0.02, shrink=0.8\n",
    ")\n",
    "cbar2.set_label(\"precipitation\", fontsize=10)\n",
    "\n",
    "mappable3 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\n",
    "mappable3.set_array(data_nuts[\"R0\"].values)\n",
    "cbar3 = fig.colorbar(\n",
    "    mappable3, ax=axes[2, :], orientation=\"vertical\", pad=0.02, shrink=0.8\n",
    ")\n",
    "cbar3.set_label(\"Model output (R0)\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also plot the population data\n",
    "# note that population is not available for every time step\n",
    "selected_times = [\"2016-01-01\", \"2017-01-01\"]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot raw and processed data for selected times\n",
    "for i, time_val in enumerate(selected_times):\n",
    "    # Filter data for current timestamp\n",
    "    current_data_pop = data_nuts[data_nuts[\"time\"] == time_val]\n",
    "    current_data_pop.plot(\n",
    "        column=\"total-population\",\n",
    "        ax=axes[i],\n",
    "        cmap=\"coolwarm\",\n",
    "        legend=False,\n",
    "        missing_kwds={\"color\": \"lightgrey\"},\n",
    "    )\n",
    "    axes[i].set_title(f\"total population - {time_val[:7]}\", pad=15)\n",
    "    axes[i].set_xlim(-10, 35)\n",
    "    axes[i].set_ylim(34, 72)\n",
    "\n",
    "# Create colorbars using ScalarMappable so they work with GeoDataFrame plots\n",
    "\n",
    "mappable1 = mpl.cm.ScalarMappable(cmap=\"coolwarm\")\n",
    "mappable1.set_array(data_nuts[\"total-population\"].values)\n",
    "cbar1 = fig.colorbar(\n",
    "    mappable1, ax=axes[:], orientation=\"vertical\", pad=0.02, shrink=0.8\n",
    ")\n",
    "cbar1.set_label(\"Total population\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heiplanet-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
